1. all_data_backend.py:

import os
import json
import hashlib
import datetime
import time
import pandas as pd
import shutil

from fastapi import FastAPI, Request, Query, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, List

from utils_gdrive import ensure_gdrive_data
from smart_file_loader import (
    load_all_parquet_tables,
    get_first_parquet_file_path,
)
from progress_manager import ProgressManager

# --- CONFIGURABLE ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

PER_FILE_MAX = 15000
TOTAL_MAX = 15000
BATCH_SIZE = 15000  # Untuk endpoint auto batch

GDRIVE_FOLDER_ID_CSVJSON = "1nDveh8erKuSFTghCHhlPEEZmu9SUo8jB"
SERVICE_ACCOUNT_JSON_PATH_CSVJSON = os.path.join(BASE_DIR, "gdrive_service_account_csvjson.json")
GDRIVE_FOLDER_ID_OTHER = "1Yx_Zeil9xoFPxS6-QZTBpwnhmaE2pSzu"
SERVICE_ACCOUNT_JSON_PATH_OTHER = os.path.join(BASE_DIR, "gdrive_service_account.json")

META_FILE = os.path.join(DATA_DIR, "csvjson_gdrive_meta.json")
META_FILES = [
    os.path.join(DATA_DIR, "csvjson_gdrive_meta.json"),
    os.path.join(DATA_DIR, "other_gdrive_meta.json"),
]
EXCLUDE_FILES = {'file_progress.json', 'csvjson_gdrive_meta.json', 'other_gdrive_meta.json'}
ALLOWED_STATUS = {"active", "new", "changed", "done"}

pm = ProgressManager(DATA_DIR)

def serialize_for_json(obj):
    if isinstance(obj, (datetime.date, datetime.datetime)):
        return obj.isoformat()
    if isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    return str(obj)

def get_valid_meta_files(meta_file=META_FILE, allowed_status=ALLOWED_STATUS):
    """Ambil semua file parquet dari meta yang statusnya valid."""
    files = []
    if not os.path.exists(meta_file):
        return []
    with open(meta_file, "r", encoding="utf-8") as f:
        meta = json.load(f)
    for m in meta:
        fname = m.get("saved_name")
        status = m.get("status", "")
        if fname and status in allowed_status and fname.lower().endswith(".parquet"):
            files.append(fname)
    return files

def get_files_with_total(pm, meta_file=META_FILE):
    files = []
    valid_files = get_valid_meta_files(meta_file)
    for fname in valid_files:
        meta = pm.get_file_progress(fname)
        total = meta.get("total", 0)
        processed = meta.get("processed", 0)
        files.append({"name": fname, "total": total, "processed": processed})
    files.sort(key=lambda x: x["total"])
    return files

def ambil_data(file_name, offset, limit):
    fpath = os.path.join(DATA_DIR, file_name)
    df = pd.read_parquet(fpath)
    batch = df.iloc[offset:offset+limit].to_dict(orient="records")
    return batch

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def root():
    print("[DEBUG] root called")
    return {"message": "FastAPI backend is running!"}

@app.get("/file_row_status")
def file_row_status(
    file: Optional[str] = Query(None, description="Nama file (filter)"),
    is_estimated: Optional[bool] = Query(None, description="True=estimasi, False=real count"),
):
    try:
        progress = pm.get_all_progress()
        result = []
        for fname, entry in progress.items():
            if file and fname != file:
                continue
            if is_estimated is not None and entry.get("is_estimated", True) != is_estimated:
                continue
            result.append({
                "file": fname,
                "total": entry.get("total", 0),
                "is_estimated": entry.get("is_estimated", True),
                "processed": entry.get("processed", 0)
            })
        return result
    except Exception as e:
        print(f"[file_row_status][ERROR] {e}")
        return []

@app.post("/trigger_gdrive_sync")
def trigger_gdrive_sync():
    log = []
    try:
        print("[DEBUG] trigger_gdrive_sync: Syncing csvjson folder")
        ensure_gdrive_data(
            folder_id=GDRIVE_FOLDER_ID_CSVJSON,
            data_dir=DATA_DIR,
            service_account_json_path=SERVICE_ACCOUNT_JSON_PATH_CSVJSON,
            meta_prefix="csvjson"
        )
        log.append("Synced csvjson folder")
    except Exception as e:
        log.append(f"Failed to sync csvjson: {e}")
        print(f"[DEBUG] trigger_gdrive_sync: Failed to sync csvjson: {e}")
    try:
        print("[DEBUG] trigger_gdrive_sync: Syncing other folder")
        ensure_gdrive_data(
            folder_id=GDRIVE_FOLDER_ID_OTHER,
            data_dir=DATA_DIR,
            service_account_json_path=SERVICE_ACCOUNT_JSON_PATH_OTHER,
            meta_prefix="other"
        )
        log.append("Synced other folder")
    except Exception as e:
        log.append(f"Failed to sync other: {e}")
        print(f"[DEBUG] trigger_gdrive_sync: Failed to sync other: {e}")
    print(f"[DEBUG] trigger_gdrive_sync: log={log}")
    return JSONResponse({"status": "done", "log": log})

@app.post("/trigger_download_missing_files")
def trigger_download_missing_files():
    """
    Mendownload file dari GDrive untuk entry meta yang status-nya missing_local, new, atau changed.
    Berjalan untuk semua meta file di META_FILES.
    """
    from download_gdrive_files import download_missing_files

    log = []
    results = []
    folder_configs = [
        {
            "meta_prefix": "csvjson",
            "data_dir": DATA_DIR,
            "service_account_json_path": SERVICE_ACCOUNT_JSON_PATH_CSVJSON,
        },
        {
            "meta_prefix": "other",
            "data_dir": DATA_DIR,
            "service_account_json_path": SERVICE_ACCOUNT_JSON_PATH_OTHER,
        }
    ]
    for conf in folder_configs:
        try:
            res = download_missing_files(
                meta_prefix=conf["meta_prefix"],
                data_dir=conf["data_dir"],
                service_account_json_path=conf["service_account_json_path"]
            )
            results.append(res)
            log.append(f"Downloaded for {conf['meta_prefix']}")
        except Exception as e:
            results.append({"error": str(e)})
            log.append(f"Failed to download for {conf['meta_prefix']}: {e}")
    return {"status": "done", "log": log, "results": results}

@app.post("/sync_progress")
def sync_progress():
    print("[DEBUG] /sync_progress called")
    pm.sync_progress_with_files()
    print("[DEBUG] /sync_progress finished")
    return {"status": "synced"}

@app.post("/run_batch_orchestrator")
def run_batch_orchestrator(background_tasks: BackgroundTasks):
    print("[DEBUG] /run_batch_orchestrator called")
    def _run():
        print("[DEBUG] /run_batch_orchestrator background main_loop start")
        from agentic_batch_orchestrator import main_loop
        main_loop()
        print("[DEBUG] /run_batch_orchestrator background main_loop finished")
    background_tasks.add_task(_run)
    return {"status": "started"}

def _detect_file(tname, tdict, data_dir):
    filename = tdict.get('filename') or tdict.get('file_path') or tdict.get('saved_name') or None
    if filename and os.path.basename(filename):
        filename = os.path.basename(filename)
    else:
        candidates = []
        for f in get_valid_meta_files():
            fname, ext = os.path.splitext(f)
            if fname == tname or f == tname or f.startswith(tname):
                candidates.append(f)
        filename = candidates[0] if candidates else tname
    print(f"[DEBUG] _detect_file: tname={tname}, detected filename={filename}")
    return filename

def collect_tabular_data(data_dir, only_table=None, include_progress=True, only_processed=True):
    print(f"[DEBUG] collect_tabular_data: only_table={only_table}, only_processed={only_processed}")
    tables_parquet = load_all_parquet_tables(data_dir)
    print(f"[DEBUG] collect_tabular_data: loaded tables_parquet={list(tables_parquet.keys())}")
    file_entries = []
    keys = [only_table] if only_table else list(tables_parquet.keys())
    for tname in keys:
        tdict = tables_parquet.get(tname)
        if not tdict:
            continue
        filename = _detect_file(tname, tdict, data_dir)
        if filename in EXCLUDE_FILES:
            print(f"[DEBUG] collect_tabular_data: skipping excluded file {filename}")
            continue
        data = tdict.get('data', [])
        if (
            data
            and isinstance(data, list)
            and all(isinstance(row, dict) for row in data)
        ):
            fpath = os.path.join(data_dir, filename)
            try:
                size_bytes = os.path.getsize(fpath)
            except Exception as e:
                print(f"[DEBUG] collect_tabular_data: os.path.getsize failed for {fpath}: {e}")
                size_bytes = len(json.dumps(data, default=str).encode("utf-8"))
            file_entries.append((tname, tdict, filename, size_bytes))
    file_entries = sorted(file_entries, key=lambda x: x[3])
    merged = []
    for tname, tdict, filename, _ in file_entries:
        data = tdict.get('data', [])
        file_prog = pm.get_file_progress(filename)
        processed = file_prog.get('processed', 0) if (file_prog and only_processed) else None
        if processed is not None and processed > 0:
            filtered_data = data[:processed]
        elif processed is not None and processed == 0:
            filtered_data = []
        else:
            filtered_data = data
        for row in filtered_data:
            row_with_file = dict(row)
            row_with_file['data_file'] = filename
            merged.append(row_with_file)
    print(f"[DEBUG] collect_tabular_data: merged data length={len(merged)}")
    return merged

def list_all_tables(data_dir):
    print(f"[DEBUG] list_all_tables called")
    tables_parquet = load_all_parquet_tables(data_dir)
    result_tables = list(tables_parquet.keys())
    print(f"[DEBUG] list_all_tables: result_tables={result_tables}")
    return result_tables

@app.get("/list_tables")
def api_list_tables():
    print("[DEBUG] api_list_tables called")
    return JSONResponse(content={"tables": list_all_tables(DATA_DIR)})

@app.get("/all_data_merge")
def api_all_data_merge(
    limit: int = Query(15000, ge=1, le=100000),
    offset: int = Query(0, ge=0),
    table: str = Query(None)
):
    print(f"[DEBUG] api_all_data_merge called: limit={limit}, offset={offset}, table={table}")
    try:
        merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
        paged_data = merged[offset:offset+limit]
        print(f"[DEBUG] api_all_data_merge: paged_data length={len(paged_data)}")
        json_compatible = json.loads(json.dumps(paged_data, default=serialize_for_json))
        return JSONResponse(content=json_compatible)
    except Exception as e:
        print(f"[all_data_merge][ERROR] {e}")
        return JSONResponse(content=[])

@app.post("/all_data_merge")
@app.put("/all_data_merge")
@app.patch("/all_data_merge")
async def api_all_data_merge_post(
    request: Request,
    limit: int = Query(15000, ge=1, le=100000),
    offset: int = Query(0, ge=0),
    table: str = Query(None)
):
    print(f"[DEBUG] api_all_data_merge_post called: limit={limit}, offset={offset}, table={table}")
    max_size = 100 * 1024 * 1024  # 100MB
    try:
        body = await request.body()
        if len(body) > max_size:
            print("[DEBUG] api_all_data_merge_post: body too large")
            raise HTTPException(status_code=413, detail="Payload too large (max 100 MB)")
        data = await request.json()
        if isinstance(data, list):
            merged = data[offset:offset+limit]
        elif isinstance(data, dict):
            if 'data' in data and isinstance(data['data'], list):
                merged = data['data'][offset:offset+limit]
            else:
                merged = [data][offset:offset+limit]
        else:
            merged = []
        if not merged:
            print("[DEBUG] api_all_data_merge_post: no data in body, fallback to local")
            raise Exception("No data in body, fallback to local")
        json_compatible = json.loads(json.dumps(merged, default=serialize_for_json))
        return JSONResponse(content=json_compatible)
    except Exception as e:
        print(f"[all_data_merge_post][ERROR] {e}")
        try:
            merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
            paged_data = merged[offset:offset+limit]
            json_compatible = json.loads(json.dumps(paged_data, default=serialize_for_json))
            return JSONResponse(content=json_compatible)
        except Exception as e2:
            print(f"[all_data_merge_post][HYBRID-FALLBACK] Fallback total failure: {e2}")
            return JSONResponse(content=[])

@app.get("/download_data")
def download_data(table: str = Query(None, description="Nama table/data yang ingin diunduh (tanpa extensi)")):
    print(f"[DEBUG] download_data called: table={table}")
    file_path, file_name, media_type = get_first_parquet_file_path(DATA_DIR, table)
    if not file_path or not os.path.exists(file_path):
        print(f"[DEBUG] download_data: file not found")
        raise HTTPException(status_code=404, detail="File data tidak ditemukan")
    print(f"[DEBUG] download_data: sending file {file_path}")
    return FileResponse(file_path, media_type=media_type, filename=file_name)

@app.post("/auto_batch_data")
def auto_batch_data():
    # --- AGENTIC PRUNE: Selalu pastikan meta sudah bersih sebelum proses batch ---
    for meta_file in META_FILES:
        agentic_prune_meta(meta_file, DATA_DIR)

    pm = ProgressManager(DATA_DIR)
    files = get_files_with_total(pm)
    total_records = sum(f["total"] for f in files)
    all_processed = sum(f["processed"] for f in files)
    batch_num = 1
    results = []

    if all_processed < total_records:
        batch_quota = BATCH_SIZE
        batch_detail = []
        for f in files:
            fname = f["name"]
            meta = pm.get_file_progress(fname)
            processed = meta.get("processed", 0)
            total = meta.get("total", 0)
            sisa = total - processed
            if sisa <= 0:
                continue
            ambil_n = min(batch_quota, sisa)
            if ambil_n > 0:
                data = ambil_data(fname, processed, ambil_n)
                pm.update_progress(
                    file_name=fname,
                    processed=processed + ambil_n,
                    last_batch=batch_num,
                    last_batch_size=ambil_n
                )
                batch_detail.append({
                    "file": fname,
                    "start": processed,
                    "count": ambil_n,
                    "end": processed + ambil_n
                })
                batch_quota -= ambil_n
                all_processed += ambil_n
            if batch_quota <= 0:
                break
        results.append({
            "batch": batch_num,
            "detail": batch_detail
        })

    return {
        "message": f"Batch diproses.",
        "progress": {f["name"]: pm.get_file_progress(f["name"]) for f in files},
        "batches": results
    }

def get_file_info(file_path):
    ext = os.path.splitext(file_path)[1].lower().lstrip('.')
    size = 0
    row_count = 0
    data_row_start = 0
    data_row_end = 0

    try:
        size = os.path.getsize(file_path)
        if ext == 'parquet':
            try:
                df = pd.read_parquet(file_path)
                row_count = len(df)
                data_row_start = 1 if row_count > 0 else 0
                data_row_end = row_count if row_count > 0 else 0
            except Exception:
                row_count = 0
                data_row_start, data_row_end = 0, 0
        else:
            row_count = 0
            data_row_start = 0
            data_row_end = 0
    except Exception as e:
        size = 0
        row_count = 0
        data_row_start = 0
        data_row_end = 0

    return size, row_count, data_row_start, data_row_end, ext

@app.get("/scan_data_folder_summary")
def scan_data_folder_summary():
    t0 = time.time()
    # Gunakan meta, bukan glob scan folder
    file_list = []
    for meta_file in META_FILES:
        if not os.path.exists(meta_file):
            continue
        with open(meta_file, "r", encoding="utf-8") as f:
            meta = json.load(f)
        for m in meta:
            fname = m.get("saved_name")
            status = m.get("status", "")
            if fname and status in ALLOWED_STATUS and fname.lower().endswith('.parquet'):
                file_path = os.path.join(DATA_DIR, fname)
                if os.path.isfile(file_path) and fname not in EXCLUDE_FILES:
                    file_list.append(file_path)
    summary = []
    total_items = 0
    total_size = 0
    for file_path in file_list:
        file_name = os.path.basename(file_path)
        size, row_count, data_row_start, data_row_end, ext = get_file_info(file_path)
        total_items += row_count
        total_size += size
        summary.append({
            "file_name": file_name,
            "file_path": file_path,
            "file_type": ext,
            "size_bytes": size,
            "row_count": row_count,
            "data_row_start": data_row_start,
            "data_row_end": data_row_end
        })
    t1 = time.time()
    return JSONResponse({
        "total_items": total_items,
        "total_files": len(summary),
        "total_size_bytes": total_size,
        "files": summary,
        "folder_path": DATA_DIR,
        "scan_duration_seconds": round(t1 - t0, 3),
        "last_scan_utc": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
    })

# =========================
# AGENTIC PRUNE META ENDPOINT
# =========================
from pydantic import BaseModel
from typing import Optional as TypingOptional, List as TypingList, Set as TypingSet

def agentic_prune_meta(meta_path, data_dir, gdrive_names: TypingOptional[TypingSet[str]] = None, keep_status={"active", "changed", "new", "done"}):
    """
    Prune meta secara agentic: hanya entry yang statusnya valid DAN file-nya benar-benar ADA (di lokal atau di GDrive).
    - Tidak hanya dedup, tapi juga hapus entry zombie/orphan.
    - Melindungi file .csv, .parquet, .parquet.meta.json, dsb.
    - gdrive_names: opsional, set file yang masih ada di GDrive (misal hasil sync terakhir).
    """
    if not os.path.exists(meta_path):
        print(f"[AGENTIC][PRUNE][WARNING] Meta file {meta_path} tidak ditemukan.")
        return {"status": "failed", "reason": f"Meta file {meta_path} tidak ditemukan."}

    # 1. Ambil semua file nyata di data_dir (case-insensitive)
    real_files = set(f.lower() for f in os.listdir(data_dir))
    # 2. Baca meta
    with open(meta_path, "r", encoding="utf-8") as f:
        try:
            meta = json.load(f)
        except Exception as e:
            msg = f"[AGENTIC][PRUNE][ERROR] Gagal load meta: {e}"
            print(msg)
            return {"status": "failed", "reason": msg}
    # 3. Bangun ulang meta: hanya entry yang status OK dan file-nya nyata (lokal atau GDrive)
    pruned_meta = []
    for entry in meta:
        fname = (entry.get("saved_name") or entry.get("name") or "").lower()
        status = entry.get("status", "")
        keep = False
        if status in keep_status:
            if fname in real_files:
                keep = True
            elif gdrive_names and fname in (n.lower() for n in gdrive_names):
                keep = True
        if keep:
            pruned_meta.append(entry)
    # 4. Rewrite meta (overwrite, anti-append, anti-zombie)
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(pruned_meta, f, indent=2, ensure_ascii=False)
    print(f"[AGENTIC][PRUNE] Meta file pruned+rewrite: {meta_path} ({len(meta)} → {len(pruned_meta)} entry).")
    return {
        "status": "success",
        "meta_path": meta_path,
        "pruned_count": len(pruned_meta),
        "original_count": len(meta)
    }

class PruneRequest(BaseModel):
    data_dir: str
    meta_path: str
    gdrive_names: TypingOptional[TypingList[str]] = None

@app.post("/prune_agentic_meta")
def prune_agentic_meta_api(req: PruneRequest):
    """
    HTTP API: POST /prune_agentic_meta
    Body: {"data_dir": "...", "meta_path": "...", "gdrive_names": [...] (optional)}
    """
    gdrive_names = set(req.gdrive_names) if req.gdrive_names else None
    result = agentic_prune_meta(req.meta_path, req.data_dir, gdrive_names=gdrive_names)
    return result

# =========================
# CLEANUP ORPHAN FILES ENDPOINT
# =========================
def cleanup_orphan_files(data_dir, meta_paths, exclude_files=None):
    """
    Menghapus file orphan di data_dir yang tidak ada di semua daftar meta_paths.
    exclude_files: set nama file yang tidak ingin dihapus (misal: file meta sendiri, file progress, dsb).
    meta_paths: bisa satu string (path) atau list of string (multi-meta).
    Hanya file yang memang sudah terhapus di GDrive yang dihapus dari data_dir.
    """
    if exclude_files is None:
        exclude_files = set()
    else:
        exclude_files = set(exclude_files)
    if isinstance(meta_paths, str):
        meta_paths = [meta_paths]
    expected_files = set()
    for meta_path in meta_paths:
        if not os.path.exists(meta_path):
            continue
        with open(meta_path, "r", encoding="utf-8") as f:
            try:
                meta = json.load(f)
                expected_files.update(m.get("saved_name") for m in meta if m.get("saved_name"))
            except Exception as e:
                print(f"[CLEANUP][ERROR] meta_path {meta_path}: {e}")
    current_files = set(os.listdir(data_dir))
    protected_files = expected_files | exclude_files
    orphan_files = [f for f in current_files if f not in protected_files]
    deleted = []
    for f in orphan_files:
        file_path = os.path.join(data_dir, f)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.remove(file_path)
                deleted.append(f)
                print(f"[CLEANUP] Deleted orphan file: {f}")
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
                deleted.append(f)
                print(f"[CLEANUP] Deleted orphan folder: {f}")
        except Exception as e:
            print(f"[CLEANUP][ERROR] Failed to delete {f}: {e}")
    return deleted

@app.post("/cleanup_orphan_files")
def cleanup_orphan_files_endpoint():
    meta_paths = META_FILES
    exclude = set(EXCLUDE_FILES)
    for mp in meta_paths:
        exclude.add(os.path.basename(mp))
    deleted = cleanup_orphan_files(DATA_DIR, meta_paths, exclude_files=exclude)
    return {"status": "success", "deleted_files": deleted}

# Include modular routers if available
from upload_frontend_data import router as upload_router
app.include_router(upload_router)

from all_data_audit import router as audit_router
app.include_router(audit_router)

from universal_to_parquet_api import router as parquet_router
app.include_router(parquet_router)

# Uvicorn entry point for direct CLI run
if __name__ == "__main__":
    import uvicorn
    print("[DEBUG] __main__ starting uvicorn")
    uvicorn.run("all_data_backend:app", host="0.0.0.0", port=8000, reload=True, workers=1)

2. all_data_audit.py:

import os
import json
import hashlib
import pandas as pd
import numpy as np
import datetime
import math
import time

from fastapi import APIRouter
from fastapi.responses import JSONResponse

from progress_manager import ProgressManager
from smart_file_loader import load_all_parquet_tables

import requests

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_DIR, "csvjson_gdrive_meta.json")
PROGRESS_FILE = os.path.join(DATA_DIR, "file_progress.json")

router = APIRouter()
pm = ProgressManager(DATA_DIR)

def safe(val, default=None):
    return val if val not in [None, "", [], {}, "null", "None"] else default

def get_file_summary_map():
    try:
        resp = requests.get("http://127.0.0.1:8000/scan_data_folder_summary", timeout=5)
        if resp.ok:
            files = resp.json().get("files", [])
            return {f['file_name']: f for f in files}
    except Exception as e:
        print(f"[DEBUG] get_file_summary_map error: {e}")
    return {}

def get_auto_batch_progress_map():
    try:
        resp = requests.post("http://127.0.0.1:8000/auto_batch_data", timeout=5)
        if resp.ok:
            return resp.json().get("progress", {})
    except Exception as e:
        print(f"[DEBUG] get_auto_batch_progress_map error: {e}")
    return {}

def get_all_progress():
    try:
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"[DEBUG] Failed to load progress: {e}")
    return {}

def calc_sha256_from_file(path):
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        print(f"[DEBUG] calc_sha256_from_file: failed for path={path}, error={e}")
        return ""

def clean_json(obj):
    if isinstance(obj, dict):
        return {str(k): clean_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_json(v) for v in obj]
    elif isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        return obj
    elif isinstance(obj, (datetime.datetime, datetime.date)):
        return str(obj)
    else:
        return obj

def suggest_remediation(eda_result):
    suggestions = []
    if eda_result.get("completeness_score", 100) < 100:
        for col, missing in eda_result.get("missing_per_col", {}).items():
            if missing > 0:
                suggestions.append(f"Imputasi nilai kosong di kolom '{col}' (mean/median/mode/khusus).")
    if eda_result.get("duplicate_pct", 0) > 0:
        suggestions.append("Hapus baris duplikat.")
    for col, stat in eda_result.get("numeric", {}).items():
        n = eda_result.get("total_rows", 0)
        outlier_count = stat.get("outlier_count", 0)
        if n > 0 and (outlier_count / n) > 0.05:
            suggestions.append(f"Tangani outlier di kolom '{col}' (misal winsorizing, capping, atau hapus).")
    id_like_cols = [col for col in eda_result.get("columns", []) if col.lower().endswith("_id") or col.lower() == "id" or col.lower().endswith("code")]
    for col, stat in eda_result.get("categorical", {}).items():
        if col in id_like_cols:
            continue
        rare_pct = stat.get("rare_pct", 0)
        if rare_pct > 20:
            suggestions.append(f"Periksa value langka di kolom '{col}' (rare value {rare_pct:.2f}%).")
    if not suggestions:
        suggestions.append("Tidak ada masalah signifikan. Data sudah baik.")
    return suggestions

def advanced_eda(df: pd.DataFrame, max_freq=5):
    result = {}
    if df is None or df.empty:
        return {"error": "Empty or unreadable file."}
    n = len(df)
    result['total_rows'] = n
    result['columns'] = list(df.columns)
    result['columns_count'] = len(df.columns)
    result['duplicate_rows'] = int(df.duplicated().sum())
    result['duplicate_pct'] = round(df.duplicated().mean() * 100, 2)
    result['missing_per_col'] = df.isnull().sum().to_dict()
    result['missing_pct_per_col'] = (df.isnull().mean()*100).round(2).to_dict()

    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    result['numeric'] = {}
    outlier_fracs = []
    for col in numeric_cols:
        s = df[col].dropna()
        desc = s.describe(percentiles=[.25, .5, .75]).to_dict()
        hist = np.histogram(s, bins=10) if len(s) else ([], [])
        iqr = desc.get('75%') - desc.get('25%') if '75%' in desc and '25%' in desc else None
        outlier_count = int(((s < (desc.get('25%', 0) - 1.5 * iqr)) | (s > (desc.get('75%', 0) + 1.5 * iqr))).sum()) if iqr else 0
        frac = outlier_count / n if n else 0
        outlier_fracs.append(frac)
        skew = float(s.skew()) if len(s) > 2 else 0
        kurt = float(s.kurt()) if len(s) > 2 else 0
        zeros = int((s == 0).sum())
        neg = int((s < 0).sum())
        pos = int((s > 0).sum())
        result['numeric'][col] = {
            **desc,
            "unique": int(s.nunique()),
            "outlier_count": outlier_count,
            "hist_bins": hist[1].tolist() if len(hist[1]) else [],
            "hist_counts": hist[0].tolist() if len(hist[0]) else [],
            "skewness": skew,
            "kurtosis": kurt,
            "zero_ratio": zeros / len(s) if len(s) else 0,
            "neg_ratio": neg / len(s) if len(s) else 0,
            "pos_ratio": pos / len(s) if len(s) else 0
        }

    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    result['categorical'] = {}
    for col in cat_cols:
        s = df[col].astype(str).fillna('')
        freq = s.value_counts().head(max_freq).to_dict()
        entropy = float(pd.Series(s).value_counts(normalize=True).map(lambda p: -p * np.log2(p)).sum())
        rare = [v for v, c in pd.Series(s).value_counts().items() if c == 1]
        rare_count = len(rare)
        rare_pct = rare_count/len(s) * 100 if len(s) else 0
        result['categorical'][col] = {
            "unique": int(s.nunique()),
            "top_freq": freq,
            "entropy": entropy,
            "rare_count": rare_count,
            "rare_pct": rare_pct
        }

    time_cols = [c for c in df.columns if any(x in c.lower() for x in ['date', 'time'])]
    result['datetime'] = {}
    for col in time_cols:
        try:
            s = pd.to_datetime(df[col], errors='coerce')
            s_nonnull = s.dropna()
            if not s_nonnull.empty:
                hist = s_nonnull.dt.date.value_counts().sort_index()
                gaps = s_nonnull.sort_values().diff().dt.days.dropna()
                result['datetime'][col] = {
                    "min": str(s_nonnull.min()),
                    "max": str(s_nonnull.max()),
                    "range_days": (s_nonnull.max() - s_nonnull.min()).days,
                    "count_per_day": {str(k): v for k, v in hist.to_dict().items()},
                    "most_active_day": str(hist.idxmax()) if not hist.empty else None,
                    "gap_days_max": int(gaps.max()) if not gaps.empty else 0
                }
        except Exception:
            continue

    if numeric_cols and len(numeric_cols) > 1:
        corr = df[numeric_cols].corr().round(2).to_dict()
        result['correlation_numeric'] = corr

    result['sample_rows'] = df.sample(min(5, len(df)), random_state=42).to_dict(orient='records')

    non_missing = [1 - (result["missing_per_col"].get(col, 0) / n) for col in result["columns"]]
    completeness_score = round(sum(non_missing) / len(non_missing) * 100, 2) if non_missing else 0.0
    dupe_penalty = max(0, 1 - (result.get("duplicate_pct", 0)/100))

    outlier_penalty = 1.0
    if outlier_fracs:
        avg_outlier_frac = np.mean([f for f in outlier_fracs if f > 0.05]) if any(f > 0.05 for f in outlier_fracs) else 0
        if avg_outlier_frac > 0:
            outlier_penalty = 1 - min(avg_outlier_frac, 0.3)

    score = completeness_score/100 * dupe_penalty * outlier_penalty
    confidence_score = round(min(max(score, 0), 1) * 100, 2)

    result["completeness_score"] = completeness_score
    result["confidence_score"] = confidence_score
    result['remediation_suggestions'] = suggest_remediation(result)
    result['generated_at'] = datetime.datetime.now().isoformat()
    return result

def explain_quality_and_confidence(eda_result):
    explanations = []
    completeness = eda_result.get("completeness_score", 0)
    if completeness == 100:
        explanations.append("Tidak ada missing value.")
    elif completeness >= 90:
        explanations.append(f"Ada sedikit missing value (score {completeness}%).")
    elif completeness > 0:
        explanations.append(f"Banyak missing value (score {completeness}%).")
    else:
        explanations.append("Seluruh data hilang atau tidak terdeteksi.")

    dupe_pct = eda_result.get("duplicate_pct", 0)
    if dupe_pct == 0:
        explanations.append("Tidak ada baris duplikat.")
    elif dupe_pct < 5:
        explanations.append(f"Baris duplikat sangat sedikit ({dupe_pct:.2f}%).")
    else:
        explanations.append(f"Banyak baris duplikat ({dupe_pct:.2f}%).")

    outlier_cols = []
    for col, stat in eda_result.get("numeric", {}).items():
        outlier_count = stat.get("outlier_count", 0)
        total = eda_result.get("total_rows", 0)
        if total > 0 and outlier_count/total > 0.05:
            outlier_cols.append(f"{col} ({outlier_count} outlier)")
    if outlier_cols:
        explanations.append("Kolom numerik dengan banyak outlier: " + ", ".join(outlier_cols))
    else:
        explanations.append("Jumlah outlier pada kolom numerik normal.")

    cat_info = []
    id_like_cols = [col for col in eda_result.get("columns", []) if col.lower().endswith("_id") or col.lower() == "id" or col.lower().endswith("code")]
    for col, stat in eda_result.get("categorical", {}).items():
        if col in id_like_cols:
            continue
        rare_pct = stat.get("rare_pct", 0)
        if rare_pct > 20:
            cat_info.append(f"{col} ({rare_pct:.2f}% rare value)")
    if cat_info:
        explanations.append("Banyak value langka pada kolom kategorik: " + ", ".join(cat_info))

    confidence = eda_result.get("confidence_score", 0)
    if confidence == 100:
        explanations.append("Confidence tinggi: seluruh aspek data baik.")
    elif confidence >= 75:
        explanations.append("Confidence cukup tinggi: sedikit penalti dari missing/duplikat/outlier.")
    elif confidence >= 50:
        explanations.append("Confidence sedang: penalti dari missing/duplikat/outlier cukup signifikan.")
    elif confidence > 0:
        explanations.append("Confidence rendah: banyak masalah missing, duplikat, atau outlier.")
    else:
        explanations.append("Confidence tidak dapat dihitung.")

    return " | ".join(explanations)

def get_total_processing_time_from_meta(meta_entry):
    try:
        start = meta_entry.get("process_start_time")
        end = meta_entry.get("process_end_time")
        if start and end:
            t0 = pd.to_datetime(start)
            t1 = pd.to_datetime(end)
            return (t1 - t0).total_seconds()
        return None
    except Exception as e:
        print(f"[DEBUG] get_total_processing_time_from_meta error: {e}")
        return None

def load_meta():
    if not os.path.exists(META_FILE):
        return []
    try:
        with open(META_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"[DEBUG] Failed to load meta: {e}")
        return []

def find_meta_entry(meta_list, fname):
    for entry in meta_list:
        if entry.get("saved_name", "") == fname or entry.get("name", "") == fname:
            return entry
    return {}

@router.get("/all_data_audit")
def all_data_audit_get():
    print("[DEBUG] all_data_audit_get: called")
    meta_list = load_meta()
    progress_map = get_all_progress()
    file_summary_map = get_file_summary_map()
    batch_progress_map = get_auto_batch_progress_map()
    tables = load_all_parquet_tables(DATA_DIR)

    eda_results = {}
    all_files = []
    total_items = 0
    file_count = 0

    for fname, table in tables.items():
        fpath = os.path.join(DATA_DIR, fname)
        meta_entry = find_meta_entry(meta_list, fname)
        progress_entry = progress_map.get(fname, {})
        batch_entry = batch_progress_map.get(fname, {})
        file_summary = file_summary_map.get(fname, {})

        # --- Run EDA (prioritas utama) ---
        try:
            df = pd.DataFrame(table.get("data", []))
            if not df.empty:
                advanced_eda_result = advanced_eda(df)
                advanced_eda_result["explanation"] = explain_quality_and_confidence(advanced_eda_result)
                eda_results[fname] = advanced_eda_result
            else:
                advanced_eda_result = {}
        except Exception as e:
            advanced_eda_result = {"error": str(e)}
            eda_results[fname] = advanced_eda_result

        # --- Build merged record ---
        def from_eda(key, default=None):
            return safe(advanced_eda_result.get(key), default)

        def from_meta(key, default=None):
            return safe(meta_entry.get(key), default)

        def from_progress(key, default=None):
            return safe(progress_entry.get(key), default)

        def from_batch_progress(key, default=None):
            return safe(batch_entry.get(key), default)

        def from_file_summary(key, default=None):
            return safe(file_summary.get(key), default)

        size_bytes = from_file_summary("size_bytes", os.path.getsize(fpath) if os.path.exists(fpath) else 0)
        modified_utc = from_file_summary("file_mtime_utc",
                                         time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime(os.path.getmtime(fpath))) if os.path.exists(fpath) else "")
        sha256 = from_file_summary("sha256", calc_sha256_from_file(fpath) if os.path.exists(fpath) else "")

        total_items_file = (
            from_eda("total_rows") or
            from_meta("record_count") or
            from_meta("total_items") or
            from_file_summary("row_count") or
            0
        )
        processed_items = (
            from_progress("processed") or
            from_batch_progress("processed") or
            0
        )
        percent_processed = (
            (processed_items / total_items_file * 100) if total_items_file else 0.0
        )

        history = from_meta("history", [])
        status = from_meta("status", "unknown")
        last_status_change = from_meta("last_status_change")
        process_start_time = from_meta("process_start_time")
        process_end_time = from_meta("process_end_time")
        total_processing_time = get_total_processing_time_from_meta(meta_entry)
        last_batch = from_progress("last_batch", from_batch_progress("batch", 0))
        last_batch_size = from_progress("last_batch_size", 0)
        retry_count = from_progress("retry_count", 0)
        last_error_type = from_progress("last_error_type", "")
        consecutive_success_count = from_progress("consecutive_success_count", 0)
        quality_explanation = from_eda("explanation", "")

        file_entry = {
            "file": fname,
            "mimeType": "application/parquet",
            "size_bytes": size_bytes,
            "modified_utc": modified_utc,
            "sha256": sha256,
            "percent_processed": round(percent_processed, 2),
            "processed_items": processed_items,
            "total_items": total_items_file,
            "status": status,
            "history": history,
            "last_status_change": last_status_change,
            "process_start_time": process_start_time,
            "process_end_time": process_end_time,
            "total_processing_time": total_processing_time,
            "last_batch": last_batch,
            "last_batch_size": last_batch_size,
            "retry_count": retry_count,
            "last_error_type": last_error_type,
            "consecutive_success_count": consecutive_success_count,
            "quality_score_explanation": quality_explanation
        }
        # Fill missing fields with default, never null
        for k in file_entry:
            if file_entry[k] is None:
                file_entry[k] = 0 if isinstance(file_entry[k], (int, float)) else ""

        all_files.append(file_entry)
        total_items += int(total_items_file) if total_items_file else 0
        file_count += 1

    print(f"[DEBUG] all_data_audit_get: returning {len(all_files)} files (no nulls, fully merged, EDA-prioritized)")
    output = {
        "file_record_audit": all_files,
        "eda_audit": eda_results,
        "meta_summary": {
            "total_items": total_items,
            "file_count": file_count,
            "meta_file": META_FILE
        }
    }
    return JSONResponse(content=clean_json(output))

@router.get("/audit_table_status")
def audit_table_status():
    """
    Endpoint khusus audit: tampilkan status file, registered/unregistered, dsb.
    """
    tables = load_all_parquet_tables(DATA_DIR)
    response = []
    for fname, table in tables.items():
        response.append({
            "file_name": fname,
            "row_count": len(table["data"]),
            "in_meta": table.get("in_meta", False),
            "meta_status": "registered" if table.get("in_meta", False) else "unregistered"
        })
    return JSONResponse(content=response)

3. auto_batch_data.py:

from fastapi import APIRouter
from progress_manager import ProgressManager
import os
import pandas as pd
import json

router = APIRouter()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_DIR, "csvjson_gdrive_meta.json")
BATCH_SIZE = 15000

def agentic_prune_meta(meta_path, data_dir, keep_status={"active", "new", "changed", "done"}):
    """
    Prune meta secara agentic: hanya entry yang statusnya valid DAN file-nya benar-benar ADA (di lokal).
    Tidak hanya dedup, tapi juga hapus entry zombie/orphan.
    """
    if not os.path.exists(meta_path):
        print(f"[AGENTIC][PRUNE][WARNING] Meta file {meta_path} tidak ditemukan.")
        return {"status": "failed", "reason": f"Meta file {meta_path} tidak ditemukan."}
    real_files = set(f.lower() for f in os.listdir(data_dir))
    with open(meta_path, "r", encoding="utf-8") as f:
        try:
            meta = json.load(f)
        except Exception as e:
            print(f"[AGENTIC][PRUNE][ERROR] Gagal load meta: {e}")
            return {"status": "failed", "reason": str(e)}
    pruned_meta = []
    seen = set()
    for entry in meta:
        fname = (entry.get("saved_name") or entry.get("name") or "").lower()
        status = entry.get("status", "")
        if status in keep_status and fname in real_files and fname not in seen:
            pruned_meta.append(entry)
            seen.add(fname)
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(pruned_meta, f, indent=2, ensure_ascii=False)
    print(f"[AGENTIC][PRUNE] Meta file pruned+rewrite: {meta_path} ({len(meta)} → {len(pruned_meta)} entry).")
    return {
        "status": "success",
        "meta_path": meta_path,
        "pruned_count": len(pruned_meta),
        "original_count": len(meta)
    }

def load_meta():
    if not os.path.exists(META_FILE):
        return []
    try:
        with open(META_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"[auto_batch_data][DEBUG] Failed to load meta: {e}")
        return []

def get_files_with_total(pm, allowed_status={"active", "new", "changed", "done"}):
    """
    Ambil list file Parquet beserta jumlah recordnya dari meta agentic (REWRITE).
    Hanya file dengan status relevan yang diproses.
    Urutkan ASC dari file paling kecil ke besar.
    PATCH: Selalu isi dan update field 'total' di progress jika belum ada/0.
    """
    files = []
    meta = load_meta()
    for entry in meta:
        fname = entry.get("saved_name") or entry.get("name")
        status = entry.get("status", "")
        if not fname or not fname.lower().endswith('.parquet'):
            continue
        if status not in allowed_status:
            continue
        meta_progress = pm.get_file_progress(fname)
        total = meta_progress.get("total", 0)
        # PATCH: hitung dan update total jika kosong/0
        if not total or total == 0:
            try:
                df = pd.read_parquet(os.path.join(DATA_DIR, fname))
                total = len(df)
                meta_progress["total"] = total
                pm.update_progress(
                    file_name=fname,
                    processed=meta_progress.get("processed", 0),
                    last_batch=meta_progress.get("last_batch", 0),
                    last_batch_size=meta_progress.get("last_batch_size", None),
                    total=total,
                )
            except Exception as e:
                print(f"[auto_batch_data][DEBUG] Error reading parquet {fname}: {e}")
                continue
        processed = meta_progress.get("processed", 0)
        files.append({"name": fname, "total": total, "processed": processed})
    files.sort(key=lambda x: x["total"])
    return files

def ambil_data(file_name, offset, limit):
    """
    Mengambil data batch dari file Parquet secara efisien.
    """
    fpath = os.path.join(DATA_DIR, file_name)
    df = pd.read_parquet(fpath)
    batch_df = df.iloc[offset:offset+limit]
    return batch_df.to_dict(orient="records")

@router.post("/auto_batch_data")
def auto_batch_data():
    # AGENTIC PATCH: SELALU PRUNE META SEBELUM BATCH!
    agentic_prune_meta(META_FILE, DATA_DIR)

    pm = ProgressManager(DATA_DIR)
    batch_num = 1
    results = []

    while True:
        files = get_files_with_total(pm)  # Refresh list & progress setiap batch
        total_records = sum(f["total"] for f in files)
        all_processed = sum(f["processed"] for f in files)
        if all_processed >= total_records or total_records == 0:
            break

        batch_quota = BATCH_SIZE
        batch_detail = []
        for f in files:
            fname = f["name"]
            meta_progress = pm.get_file_progress(fname)
            processed = meta_progress.get("processed", 0)
            total = meta_progress.get("total", 0)
            sisa = total - processed
            if sisa <= 0:
                continue
            ambil_n = min(batch_quota, sisa)
            if ambil_n > 0:
                data = ambil_data(fname, processed, ambil_n)
                pm.update_progress(
                    file_name=fname,
                    processed=processed + ambil_n,
                    last_batch=batch_num,
                    last_batch_size=ambil_n,
                    total=total
                )
                batch_detail.append({
                    "file": fname,
                    "start": processed,
                    "count": ambil_n,
                    "end": processed + ambil_n
                })
                batch_quota -= ambil_n
                if batch_quota <= 0:
                    break  # BATCH_SIZE quota habis, batch selesai
        if batch_detail:
            results.append({
                "batch": batch_num,
                "detail": batch_detail
            })
            batch_num += 1
        else:
            break

    # Setelah selesai, refresh files untuk status akhir
    files = get_files_with_total(pm)
    return {
        "message": f"Semua file status ['active', 'new', 'changed', 'done'] sudah diproses dalam {batch_num-1} batch.",
        "progress": {f["name"]: pm.get_file_progress(f["name"]) for f in files},
        "batches": results
    }

4. batch_controller.py:

import os
import json
import hashlib
import time
from typing import List, Dict, Tuple

import pandas as pd
from progress_manager import ProgressManager
# from batch_agent_experta import get_batch_plan  # Optional: enable if needed
# from row_estimator import estimate_csv_rows    # Optional: enable if needed

# --- CONFIGURABLE LIMITS ---
START_BATCH_SIZE = 15000
MAX_BATCH_SIZE = 15000
MIN_BATCH_SIZE = 15000
CONSECUTIVE_SUCCESS_TO_INCREASE = 3

TIME_FAST = 5    # seconds
TIME_SLOW = 60   # seconds

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_DIR, "csvjson_gdrive_meta.json")
pm = ProgressManager(DATA_DIR)

def agentic_prune_meta(meta_path, data_dir, keep_status={"active", "new", "changed", "done"}):
    """
    Prune meta secara agentic: hanya entry yang statusnya valid DAN file-nya benar-benar ADA (di lokal).
    Tidak hanya dedup, tapi juga hapus entry zombie/orphan.
    """
    if not os.path.exists(meta_path):
        print(f"[AGENTIC][PRUNE][WARNING] Meta file {meta_path} tidak ditemukan.")
        return {"status": "failed", "reason": f"Meta file {meta_path} tidak ditemukan."}
    real_files = set(f.lower() for f in os.listdir(data_dir))
    with open(meta_path, "r", encoding="utf-8") as f:
        try:
            meta = json.load(f)
        except Exception as e:
            print(f"[AGENTIC][PRUNE][ERROR] Gagal load meta: {e}")
            return {"status": "failed", "reason": str(e)}
    pruned_meta = []
    seen = set()
    for entry in meta:
        fname = (entry.get("saved_name") or entry.get("name") or "").lower()
        status = entry.get("status", "")
        if status in keep_status and fname in real_files and fname not in seen:
            pruned_meta.append(entry)
            seen.add(fname)
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(pruned_meta, f, indent=2, ensure_ascii=False)
    print(f"[AGENTIC][PRUNE] Meta file pruned+rewrite: {meta_path} ({len(meta)} → {len(pruned_meta)} entry).")
    return {
        "status": "success",
        "meta_path": meta_path,
        "pruned_count": len(pruned_meta),
        "original_count": len(meta)
    }

def calc_sha256_from_file(path):
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] calc_sha256_from_file failed: {e}")
        return ""

def load_meta():
    if not os.path.exists(META_FILE):
        return []
    try:
        with open(META_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] Failed to load meta: {e}")
        return []

def get_file_info_from_meta(status_allowed={"active", "new", "changed", "done"}):
    """
    Ambil info file parquet dari meta agentic (REWRITE), filter hanya status relevan.
    """
    meta = load_meta()
    info_list = []
    for entry in meta:
        fname = entry.get("saved_name") or entry.get("name")
        status = entry.get("status", "")
        if not fname or not fname.lower().endswith('.parquet'):
            continue
        if status not in status_allowed:
            continue
        fpath = os.path.join(DATA_DIR, fname)
        try:
            size_bytes = os.path.getsize(fpath)
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] get_file_info os.path.getsize failed for {fname}: {e}")
            size_bytes = 0
        sha256 = calc_sha256_from_file(fpath)
        try:
            modified_time = str(os.path.getmtime(fpath))
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] get_file_info os.path.getmtime failed for {fname}: {e}")
            modified_time = ""
        # PATCH: total_items dari progress_manager supaya selalu update
        progress_entry = pm.get_file_progress(fname)
        total_items = progress_entry.get("total", 0)
        info_list.append({
            "file": fname,
            "size_bytes": size_bytes,
            "total_items": total_items,
            "sha256": sha256,
            "modified_time": modified_time
        })
        print(f"[DEBUG] File Info: {fname}, size: {size_bytes}, total: {total_items}, sha256: {sha256}, modified: {modified_time}")
    print(f"[DEBUG] get_file_info_from_meta: info_list={info_list}")
    return info_list

def build_experta_file_status(file_info, progress):
    print(f"[DEBUG] build_experta_file_status called")
    status_list = []
    for info in file_info:
        fname = info["file"]
        entry = progress.get(fname, {})
        processed = entry.get("processed", 0) if isinstance(entry, dict) else 0
        last_batch_size = entry.get("last_batch_size", START_BATCH_SIZE)
        status_list.append({
            "name": fname,
            "size": info["total_items"],
            "total": info["total_items"],
            "processed": processed,
            "last_batch_size": last_batch_size
        })
        print(f"[DEBUG] Experta Status: name={fname}, size={info['total_items']}, total={info['total_items']}, processed={processed}, batch_size={last_batch_size}")
    print(f"[DEBUG] build_experta_file_status: status_list={status_list}")
    return status_list

def experta_batch_distributor(file_info, progress):
    """Distribusi batch per file, batch size adaptif."""
    print(f"[DEBUG] experta_batch_distributor called")
    file_status_list = build_experta_file_status(file_info, progress)
    allocations = []
    for status in file_status_list:
        fname = status["name"]
        processed = status["processed"]
        total = status["total"]
        batch_size = status["last_batch_size"]
        remain = max(total - processed, 0)
        alloc = min(batch_size, remain)
        allocations.append((fname, alloc))
        print(f"[DEBUG] Plan: {fname} batch_size={batch_size}, alloc={alloc}, remain={remain}")
    return allocations

def simulate_batch_process(file_name, start_idx, end_idx):
    print(f"[DEBUG] simulate_batch_process called: {file_name} idx {start_idx}-{end_idx}")
    # Simulasi error jika file_name mengandung "error" dan batch > 1000
    if "error" in file_name and (end_idx - start_idx) > 1000:
        print(f"[DEBUG] simulate_batch_process: simulated error (timeout) for {file_name}")
        return False, "timeout"
    return True, None

def process_file_batch(file_name, start_idx, end_idx, batch_size, progress_entry):
    print(f"[BATCH] Proses {file_name} idx {start_idx}-{end_idx}, batch_size={batch_size}")
    t0 = time.time()
    try:
        fpath = os.path.join(DATA_DIR, file_name)
        total_items = progress_entry.get("total", 0)
        try:
            df = pd.read_parquet(fpath)
            batch_rows = df.iloc[start_idx:end_idx]
        except Exception as e:
            print(f"[BATCH][ERROR] Gagal baca parquet {file_name}: {e}")
            batch_rows = None

        success, error_type = simulate_batch_process(file_name, start_idx, end_idx)
        t1 = time.time()
        elapsed = t1 - t0

        if success:
            consecutive_success_count = progress_entry.get("consecutive_success_count", 0) + 1
            if elapsed < TIME_FAST:
                new_batch_size = min(batch_size * 2, MAX_BATCH_SIZE)
                print(f"[ADAPTIVE] SUCCESS & FAST ({elapsed:.2f}s) - Doubling batch_size to {new_batch_size}")
            elif elapsed < TIME_SLOW:
                new_batch_size = batch_size
                print(f"[ADAPTIVE] SUCCESS & NORMAL ({elapsed:.2f}s) - Keep batch_size {batch_size}")
            else:
                new_batch_size = max(batch_size // 2, MIN_BATCH_SIZE)
                print(f"[ADAPTIVE] SUCCESS BUT SLOW ({elapsed:.2f}s) - Reduce batch_size to {new_batch_size}")
                consecutive_success_count = 0  # Reset count if lambat

            pm.update_progress(
                file_name,
                processed=end_idx,
                last_batch=progress_entry.get("last_batch", 0)+1,
                last_batch_size=new_batch_size,
                retry_count=0,
                last_error_type=None,
                consecutive_success_count=consecutive_success_count
            )
            print(f"[PROGRESS] {file_name}: processed={end_idx}, total={total_items}, elapsed={elapsed:.2f}s")
            return True, new_batch_size
        else:
            print(f"[ERROR] Batch {file_name} idx {start_idx}-{end_idx} FAILED: {error_type}")
            new_batch_size = max(batch_size // 2, MIN_BATCH_SIZE)
            pm.update_progress(
                file_name,
                processed=progress_entry.get("processed", 0),
                last_batch=progress_entry.get("last_batch", 0),
                last_batch_size=new_batch_size,
                retry_count=1,
                last_error_type=error_type,
                consecutive_success_count=0
            )
            print(f"[PROGRESS] {file_name}: processed={progress_entry.get('processed', 0)}, total={total_items}, last_error={error_type}, batch_size={new_batch_size}")
            return False, new_batch_size
    except Exception as e:
        print(f"[HYBRID-FALLBACK][EXCEPTION] {file_name} idx {start_idx}-{end_idx} exception: {e}")
        try:
            new_batch_size = max(batch_size // 2, MIN_BATCH_SIZE)
            pm.update_progress(
                file_name,
                processed=progress_entry.get("processed", 0),
                last_batch=progress_entry.get("last_batch", 0),
                last_batch_size=new_batch_size,
                retry_count=1,
                last_error_type="exception",
                consecutive_success_count=0
            )
            print(f"[PROGRESS] {file_name}: processed={progress_entry.get('processed', 0)}, total={progress_entry.get('total', 'unknown')}, last_error=exception, batch_size={new_batch_size}")
        except Exception as e2:
            print(f"[HYBRID-FALLBACK][ERROR] Failed to update progress after exception: {e2}")
        return False, batch_size

def all_files_done(progress, file_info):
    file_names = {info["file"] for info in file_info}
    for fname, entry in progress.items():
        if fname not in file_names:
            continue  # Only consider files from meta (filtered)
        total = entry.get("total", 0)
        processed = entry.get("processed", 0)
        if total > 0 and processed < total:
            return False
    return True

def run_batch_controller():
    print("[DEBUG] run_batch_controller: mulai agentic_prune_meta()")
    try:
        agentic_prune_meta(META_FILE, DATA_DIR)
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] prune_meta failed: {e}")

    print("[DEBUG] run_batch_controller: mulai sync_progress_with_files()")
    try:
        pm.sync_progress_with_files()
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] sync_progress_with_files failed: {e}")

    round_num = 1
    while True:
        print(f"[DEBUG] run_batch_controller: mulai batch round {round_num}")
        file_info = get_file_info_from_meta()
        print(f"[DEBUG] run_batch_controller: file_info={file_info}")
        try:
            progress = pm.get_all_progress()
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] get_all_progress failed in run_batch_controller: {e}")
            progress = {}
        print(f"[DEBUG] run_batch_controller: progress={progress}")

        allocations = experta_batch_distributor(file_info, progress)
        print("Batch allocation this round (adaptive):")
        any_progress = False
        for fname, alloc in allocations:
            print(f"  {fname}: {alloc}")
        for fname, alloc in allocations:
            print(f"[DEBUG] Looping allocation: {fname}, alloc={alloc}")
            if alloc <= 0:
                continue
            entry = progress.get(fname, {})
            print(f"[DEBUG] Entry {fname}: {entry}")
            processed = entry.get("processed", 0)
            total = entry.get("total", 0)
            batch_size = entry.get("last_batch_size", START_BATCH_SIZE)
            start_idx = processed
            end_idx = min(processed + alloc, total)
            print(f"[DEBUG] Akan proses {fname}: {start_idx}-{end_idx} (batch_size={batch_size})")
            ok, new_batch_size = process_file_batch(fname, start_idx, end_idx, batch_size, entry)
            any_progress = True

            try:
                entry = pm.get_file_progress(fname)
                print(f"[DEBUG] Setelah process_file_batch {fname}: {entry}")
            except Exception as e2:
                print(f"[HYBRID-FALLBACK][ERROR] Error in post-batch update for {fname}: {e2}")

        try:
            progress = pm.get_all_progress()
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] get_all_progress failed in run_batch_controller: {e}")
            progress = {}
        if all_files_done(progress, file_info):
            print("[DEBUG] Semua file sudah selesai dibatch.")
            break
        if not any_progress:
            print("[DEBUG] Tidak ada file tersisa untuk diproses pada round ini. Batch selesai.")
            break
        round_num += 1

if __name__ == "__main__":
    print("[DEBUG] __main__ run_batch_controller")
    try:
        run_batch_controller()
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] run_batch_controller failed: {e}")

5. download_gdrive_files.py:

import os
import json
from datetime import datetime, timezone
from fastapi import APIRouter
from googleapiclient.discovery import build
from google.oauth2 import service_account
from googleapiclient.http import MediaIoBaseDownload

router = APIRouter()

# --- CONFIGURABLE ---
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
DEFAULT_META_PREFIX = "csvjson"
SERVICE_ACCOUNT_JSON = os.path.join(os.path.dirname(os.path.abspath(__file__)), "gdrive_service_account_csvjson.json")
GDRIVE_FOLDER_ID = "1nDveh8erKuSFTghCHhlPEEZmu9SUo8jB"

def utc_now_iso():
    return datetime.now(timezone.utc).isoformat()

def load_meta(meta_path):
    if not os.path.exists(meta_path):
        return []
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"[DOWNLOAD][META LOAD ERROR] {e}")
        return []

def save_meta_agentic(meta_path, meta):
    """
    AGENTIC: Rebuild meta as deduped, sorted by saved_name, anti-zombie, anti-append.
    """
    unique = {}
    for entry in meta:
        fname = entry.get("saved_name")
        if fname:
            # Always keep the latest/last info
            unique[fname] = entry
    meta_clean = list(unique.values())
    # Optional: sort alphabetically for deterministic file
    meta_clean.sort(key=lambda x: x.get("saved_name", ""))
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta_clean, f, indent=2)
    print(f"[META][AGENTIC][REWRITE] Meta clean rewritten: {meta_path} ({len(meta_clean)} entries)")
    return meta_clean

def update_meta_entry(meta, fname, new_info: dict):
    """
    Find and update a meta entry by saved_name. If not found, do nothing (no append!).
    """
    updated = False
    for entry in meta:
        if entry.get("saved_name") == fname:
            entry.update(new_info)
            updated = True
            break
    return updated

def build_gdrive_service(service_account_json_path):
    SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
    creds = service_account.Credentials.from_service_account_file(service_account_json_path, scopes=SCOPES)
    return build('drive', 'v3', credentials=creds)

def download_file(service, file_id, local_path):
    request = service.files().get_media(fileId=file_id)
    with open(local_path, "wb") as f:
        downloader = MediaIoBaseDownload(f, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
    print(f"[DOWNLOAD] Downloaded: {local_path}")

def download_missing_files(meta_prefix=DEFAULT_META_PREFIX, data_dir=DATA_DIR, service_account_json_path=SERVICE_ACCOUNT_JSON):
    """
    Download file dari GDrive untuk entry meta yang status-nya missing_local, new, changed.
    Update meta setelah selesai download (AGENTIC: NO APPEND, rewrite/overwrite only).
    """
    meta_path = os.path.join(data_dir, f"{meta_prefix}_gdrive_meta.json")
    meta = load_meta(meta_path)
    if not meta:
        print("[DOWNLOAD] Meta file kosong/tidak ditemukan. Tidak ada file yang diunduh.")
        return {"status": "no-meta", "downloaded": [], "skipped": []}
    service = build_gdrive_service(service_account_json_path)
    downloaded, skipped = [], []
    now = utc_now_iso()
    for entry in meta:
        status = entry.get("status", "")
        if status in ["missing_local", "new", "changed"]:
            file_id = entry.get("id")
            file_name = entry.get("saved_name")
            if not file_id or not file_name:
                skipped.append(file_name or str(entry))
                continue
            local_path = os.path.join(data_dir, file_name)
            try:
                download_file(service, file_id, local_path)
                stat = os.stat(local_path)
                new_info = {
                    "local_exists": True,
                    "local_modified_time": datetime.utcfromtimestamp(stat.st_mtime).isoformat() + "Z",
                    "local_size": stat.st_size,
                    "status": "active",
                    "status_reason": "synced",
                    "last_status_change": now,
                    "process_end_time": now,
                }
                update_meta_entry(meta, file_name, new_info)
                downloaded.append(file_name)
            except Exception as e:
                print(f"[DOWNLOAD][ERROR] Failed to download {file_name}: {e}")
                skipped.append(file_name)
        else:
            skipped.append(entry.get("saved_name"))
    # AGENTIC: Simpan meta setelah update, always dedup/overwrite, never append
    save_meta_agentic(meta_path, meta)
    print(f"[DOWNLOAD] Selesai. Downloaded: {downloaded}, Skipped: {skipped}")
    return {
        "status": "success",
        "downloaded": downloaded,
        "skipped": skipped
    }

@router.post("/trigger_download_missing_files")
def trigger_download_missing_files_endpoint():
    """
    Endpoint untuk mendownload file yang statusnya missing_local, new, atau changed berdasarkan meta clean.
    """
    result = download_missing_files()
    return result

if __name__ == "__main__":
    # Untuk pemakaian manual/CLI
    result = download_missing_files()
    print(result)

5. progress_manager.py:

import os
import json
from filelock import FileLock

class ProgressManager:
    """
    Manajemen file_progress.json: baca, tulis, update, recovery, dan query status file.
    Fully agentic: sinkronisasi penuh ke meta (csvjson_gdrive_meta.json) yang sudah di-REWRITE (overwrite, anti-append, anti-zombie).
    Process-safe (filelock) untuk multi-batch/worker.
    PATCHED: Hanya mendukung file Parquet.
    PATCH SMART: Hanya file status "active", "new", "changed", "done" yang dimonitor progress-nya.
    PATCH AGENTIC: Tidak pernah append/track file zombie/orphan.
    PATCH: get_file_progress dan get_all_progress selalu update total, sinkron ke meta.
    PATCH: Tidak pernah reset progress otomatis jika file valid kosong.
    """
    ALLOWED_STATUS = {"active", "new", "changed", "done"}

    def __init__(self, data_dir=None, progress_file=None, meta_file=None):
        if data_dir is None:
            data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
        if progress_file is None:
            progress_file = os.path.join(data_dir, "file_progress.json")
        if meta_file is None:
            meta_file = os.path.join(data_dir, "csvjson_gdrive_meta.json")
        self.data_dir = data_dir
        self.progress_file = progress_file
        self.meta_file = meta_file
        self.lock = FileLock(self.progress_file + ".lock")
        self._cache = None
        print(f"[progress_manager][DEBUG] ProgressManager initialized with data_dir={self.data_dir}, progress_file={self.progress_file}, meta_file={self.meta_file}")

    def load_progress(self):
        with self.lock:
            if not os.path.exists(self.progress_file):
                print(f"[progress_manager][DEBUG] Progress file not found: {self.progress_file}")
                return {}
            try:
                with open(self.progress_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                self._cache = data
                print(f"[progress_manager][DEBUG] Progress loaded: {data}")
                return data
            except Exception as e:
                print(f"[progress_manager] Failed to load progress: {e}")
                return {}

    def save_progress(self, progress):
        with self.lock:
            try:
                with open(self.progress_file, "w", encoding="utf-8") as f:
                    json.dump(progress, f, indent=2)
                self._cache = progress
                print(f"[progress_manager][DEBUG] Progress saved: {progress}")
            except Exception as e:
                print(f"[progress_manager] Failed to save progress: {e}")

    def get_allowed_meta_files(self):
        """
        Ambil daftar file yang statusnya valid di meta (active, new, changed, done) dan exist di folder.
        """
        allowed = set()
        if os.path.exists(self.meta_file):
            try:
                with open(self.meta_file, "r", encoding="utf-8") as f:
                    meta = json.load(f)
                for entry in meta:
                    fname = entry.get("saved_name") or entry.get("file") or entry.get("original_name")
                    status = entry.get("status", "")
                    if fname and status in self.ALLOWED_STATUS and fname.lower().endswith('.parquet'):
                        fpath = os.path.join(self.data_dir, fname)
                        if os.path.exists(fpath):
                            allowed.add(fname)
            except Exception as e:
                print(f"[progress_manager][HYBRID-FALLBACK] get_allowed_meta_files error: {e}")
        return allowed

    def update_progress(self, file_name, processed, sha256=None, modified_time=None, last_batch=None, total=None,
                        retry_count=None, last_batch_size=None, last_error_type=None, consecutive_success_count=None, is_estimated=None):
        allowed = self.get_allowed_meta_files()
        if file_name not in allowed:
            print(f"[progress_manager][AGENTIC][WARNING] {file_name} tidak valid di meta, SKIP update_progress.")
            return  # Agentic: tidak update apapun untuk file zombie/orphan
        with self.lock:
            print(f"[progress_manager][DEBUG] update_progress called for: {file_name}")
            progress = self.load_progress()
            entry = progress.get(file_name, {})
            if sha256 and entry.get("sha256") != sha256:
                print(f"[progress_manager][DEBUG] SHA256 berubah untuk {file_name}, reset entry.")
                entry = {}
            if modified_time and entry.get("modified_time") != modified_time:
                print(f"[progress_manager][DEBUG] Modified time berubah untuk {file_name}, reset entry.")
                entry = {}
            entry["processed"] = processed
            if sha256: entry["sha256"] = sha256
            if modified_time: entry["modified_time"] = modified_time
            if last_batch is not None: entry["last_batch"] = last_batch
            if retry_count is not None: entry["retry_count"] = retry_count
            if last_batch_size is not None: entry["last_batch_size"] = last_batch_size
            if last_error_type is not None: entry["last_error_type"] = last_error_type
            if consecutive_success_count is not None: entry["consecutive_success_count"] = consecutive_success_count
            if total is not None: entry["total"] = total
            if is_estimated is not None:
                entry["is_estimated"] = is_estimated
            progress[file_name] = entry
            print(f"[progress_manager][DEBUG] Progress entry for {file_name}: {entry}")
            self.save_progress(progress)

    def get_total_items_from_meta(self, file_name):
        """
        Ambil jumlah total record dari csvjson_gdrive_meta.json untuk .parquet,
        fallback: jika gagal, hitung jumlah baris file .parquet (bukan csv).
        """
        try:
            if os.path.exists(self.meta_file):
                with open(self.meta_file, "r", encoding="utf-8") as f:
                    meta_data = json.load(f)
                for entry in meta_data:
                    fname = entry.get("saved_name") or entry.get("file") or entry.get("original_name")
                    status = entry.get("status", "")
                    if fname == file_name and status in self.ALLOWED_STATUS:
                        # Prefer record_count/total_items
                        return entry.get("record_count") or entry.get("total_items") or 0
            # --- PATCH: fallback ke file parquet ---
            parquet_path = os.path.join(self.data_dir, file_name)
            if os.path.exists(parquet_path) and file_name.lower().endswith('.parquet'):
                import pandas as pd
                try:
                    df = pd.read_parquet(parquet_path)
                    return len(df)
                except Exception as e2:
                    print(f"[progress_manager][PARQUET-FALLBACK] Gagal hitung baris file {file_name}: {e2}")
                    return 0
            return 0
        except Exception as e:
            print(f"[progress_manager][HYBRID-FALLBACK] get_total_items_from_meta error: {e}")
            return 0

    def get_file_progress(self, file_name):
        allowed = self.get_allowed_meta_files()
        if file_name not in allowed:
            print(f"[progress_manager][DEBUG] get_file_progress: {file_name} tidak di meta/allowed status, return kosong.")
            return {}
        progress = self.load_progress()
        result = progress.get(file_name, {}).copy()
        try:
            total = self.get_total_items_from_meta(file_name)
            # ADVANCE PATCH: jika total kosong/0, hitung dari Parquet dan update progress
            if not total or total == 0:
                parquet_path = os.path.join(self.data_dir, file_name)
                if os.path.exists(parquet_path) and file_name.lower().endswith('.parquet'):
                    import pandas as pd
                    try:
                        df = pd.read_parquet(parquet_path)
                        total = len(df)
                        result["total"] = total
                        # Patch: auto-update progress if needed
                        self.update_progress(
                            file_name=file_name,
                            processed=result.get("processed", 0),
                            last_batch=result.get("last_batch", 0),
                            last_batch_size=result.get("last_batch_size", None),
                            total=total,
                        )
                    except Exception as e2:
                        print(f"[progress_manager][PARQUET-FALLBACK] Gagal hitung baris file {file_name}: {e2}")
                        total = 0
            result["total"] = total
            result["is_estimated"] = False
        except Exception as e:
            print(f"[progress_manager][HYBRID-FALLBACK] get_file_progress fallback: {e}")
            result["total"] = 0
            result["is_estimated"] = True
        processed = result.get("processed", 0)
        total_items = result.get("total", 0)
        if total_items and total_items > 0:
            result["percent_processed"] = round((processed / total_items) * 100, 2)
        else:
            result["percent_processed"] = None
        print(f"[progress_manager][DEBUG] get_file_progress for {file_name}: {result}")
        return result

    def reset_progress(self, file_name):
        allowed = self.get_allowed_meta_files()
        if file_name not in allowed:
            print(f"[progress_manager][AGENTIC][WARNING] {file_name} tidak valid di meta, SKIP reset_progress.")
            return
        with self.lock:
            print(f"[progress_manager][DEBUG] reset_progress called for: {file_name}")
            progress = self.load_progress()
            if file_name in progress:
                progress[file_name] = {}
                self.save_progress(progress)
                print(f"[progress_manager][DEBUG] Progress reset for {file_name}")

    def get_all_progress(self):
        allowed = self.get_allowed_meta_files()
        progress = self.load_progress()
        all_result = {}
        for fname in allowed:
            entry = progress.get(fname, {}).copy()
            total = self.get_total_items_from_meta(fname)
            entry["total"] = total
            entry["is_estimated"] = False
            processed = entry.get("processed", 0)
            if total and total > 0:
                entry["percent_processed"] = round((processed / total) * 100, 2)
            else:
                entry["percent_processed"] = None
            all_result[fname] = entry
        print(f"[progress_manager][DEBUG] get_all_progress (meta only): {all_result}")
        return all_result

    def remove_file_progress(self, file_name):
        with self.lock:
            print(f"[progress_manager][DEBUG] remove_file_progress called for: {file_name}")
            progress = self.load_progress()
            if file_name in progress:
                del progress[file_name]
                self.save_progress(progress)
                print(f"[progress_manager][DEBUG] Progress entry removed for {file_name}")

    def delete_progress(self, file_name):
        # Alias for remove_file_progress for consistency with sync tools
        self.remove_file_progress(file_name)

    def sync_progress_with_files(self, scan_result=None):
        with self.lock:
            print("[progress_manager][DEBUG] sync_progress_with_files called")
            allowed = self.get_allowed_meta_files()
            progress = self.load_progress()
            # Hanya file yang statusnya allowed di meta yang boleh progress-nya hidup
            new_progress = {}
            for fname in allowed:
                if fname not in progress:
                    print(f"[progress_manager][DEBUG] File baru terdeteksi: {fname}, entry progress dibuat otomatis.")
                    new_progress[fname] = {
                        "processed": 0,
                        "last_batch": 0,
                        "retry_count": 0,
                        "last_batch_size": None,
                        "last_error_type": None,
                        "consecutive_success_count": 0,
                    }
                else:
                    new_progress[fname] = progress[fname]
            removed_files = set(progress.keys()) - allowed
            for fname in removed_files:
                print(f"[progress_manager][DEBUG] File {fname} tidak di meta/status allowed, entry progress dihapus.")
            self.save_progress(new_progress)
            print("[progress_manager][DEBUG] Progress terbaru setelah sync:", new_progress)
            return new_progress

if __name__ == "__main__":
    pm = ProgressManager()
    print("[progress_manager] Progress snapshot:", pm.get_all_progress())
    pm.update_progress("contoh.parquet", processed=100, sha256="sha256hex", modified_time="1685420000.123", last_batch=2, retry_count=1, last_batch_size=100, last_error_type="timeout", consecutive_success_count=0, is_estimated=True)
    print("[progress_manager] Progress after update:", pm.get_all_progress())
    pm.reset_progress("contoh.parquet")
    print("[progress_manager] Progress after reset:", pm.get_all_progress())
    print("[progress_manager] Sync status:", pm.sync_progress_with_files())
    print("[progress_manager] Progress after meta auto sync:", pm.get_all_progress())

6. prune_meta_agentic.py:

import os
import json
from fastapi import FastAPI, Query
from pydantic import BaseModel
from typing import List, Optional, Set

app = FastAPI()

def agentic_prune_meta(meta_path, data_dir, gdrive_names: Optional[Set[str]] = None, keep_status={"active", "changed", "new", "done"}):
    """
    Prune meta secara agentic: hanya entry yang statusnya valid DAN file-nya benar-benar ADA (di lokal atau di GDrive).
    - Tidak hanya dedup, tapi juga hapus entry zombie/orphan.
    - Melindungi file .csv, .parquet, .parquet.meta.json, dsb.
    - gdrive_names: opsional, set file yang masih ada di GDrive (misal hasil sync terakhir).
    """
    if not os.path.exists(meta_path):
        print(f"[AGENTIC][PRUNE][WARNING] Meta file {meta_path} tidak ditemukan.")
        return {"status": "failed", "reason": f"Meta file {meta_path} tidak ditemukan."}

    # 1. Ambil semua file nyata di data_dir (case-insensitive)
    real_files = set(f.lower() for f in os.listdir(data_dir))
    # 2. Baca meta
    with open(meta_path, "r", encoding="utf-8") as f:
        try:
            meta = json.load(f)
        except Exception as e:
            msg = f"[AGENTIC][PRUNE][ERROR] Gagal load meta: {e}"
            print(msg)
            return {"status": "failed", "reason": msg}
    # 3. Bangun ulang meta: hanya entry yang status OK dan file-nya nyata (lokal atau GDrive)
    pruned_meta = []
    for entry in meta:
        fname = (entry.get("saved_name") or entry.get("name") or "").lower()
        status = entry.get("status", "")
        keep = False
        if status in keep_status:
            if fname in real_files:
                keep = True
            elif gdrive_names and fname in (n.lower() for n in gdrive_names):
                keep = True
        if keep:
            pruned_meta.append(entry)
    # 4. Rewrite meta (overwrite, anti-append, anti-zombie)
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(pruned_meta, f, indent=2, ensure_ascii=False)
    print(f"[AGENTIC][PRUNE] Meta file pruned+rewrite: {meta_path} ({len(meta)} → {len(pruned_meta)} entry).")
    return {
        "status": "success",
        "meta_path": meta_path,
        "pruned_count": len(pruned_meta),
        "original_count": len(meta)
    }

class PruneRequest(BaseModel):
    data_dir: str
    meta_path: str
    gdrive_names: Optional[List[str]] = None

@app.post("/prune_agentic_meta")
def prune_agentic_meta_api(req: PruneRequest):
    """
    HTTP API: POST /prune_agentic_meta
    Body: {"data_dir": "...", "meta_path": "...", "gdrive_names": [...] (optional)}
    """
    gdrive_names = set(req.gdrive_names) if req.gdrive_names else None
    result = agentic_prune_meta(req.meta_path, req.data_dir, gdrive_names=gdrive_names)
    return result

if __name__ == "__main__":
    # CLI mode
    import sys
    if len(sys.argv) < 3:
        print("Usage: python meta_agentic_prune.py <data_dir> <meta_path>")
        exit(1)
    data_dir = sys.argv[1]
    meta_path = sys.argv[2]
    print(agentic_prune_meta(meta_path, data_dir))

7. scan_data_folder_summary.py:

from fastapi import FastAPI
from fastapi.responses import JSONResponse
import os
import json
import time

app = FastAPI()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FOLDER = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_FOLDER, "csvjson_gdrive_meta.json")
SUPPORTED_EXTENSIONS = {'csv', 'json', 'pdf', 'docx', 'parquet'}
ALLOWED_STATUS = {"active", "new", "changed", "done"}

@app.get("/scan_data_folder_summary")
def scan_data_folder_summary():
    t0 = time.time()
    summary = []
    total_items = 0
    total_size = 0
    meta_files = 0

    # Baca meta
    if not os.path.exists(META_FILE):
        return JSONResponse({
            "error": f"Meta file {META_FILE} tidak ditemukan!",
            "total_items": 0,
            "total_files": 0,
            "total_size_bytes": 0,
            "files": [],
            "folder_path": DATA_FOLDER,
            "scan_duration_seconds": 0,
            "last_scan_utc": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
        }, status_code=500)

    with open(META_FILE, "r", encoding="utf-8") as f:
        try:
            meta = json.load(f)
        except Exception as e:
            return JSONResponse({
                "error": f"Meta file {META_FILE} tidak bisa dibaca: {e}",
                "total_items": 0,
                "total_files": 0,
                "total_size_bytes": 0,
                "files": [],
                "folder_path": DATA_FOLDER,
                "scan_duration_seconds": 0,
                "last_scan_utc": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
            }, status_code=500)
    
    for entry in meta:
        status = entry.get("status", "")
        if status not in ALLOWED_STATUS:
            continue  # Skip file yang sudah orphan, deleted, atau corrupt
        file_name = entry.get("saved_name") or entry.get("name")
        if not file_name:
            continue
        ext = os.path.splitext(file_name)[1].lstrip('.').lower()
        if ext not in SUPPORTED_EXTENSIONS:
            continue
        file_path = os.path.join(DATA_FOLDER, file_name)
        size = 0
        if os.path.exists(file_path):
            try:
                size = os.path.getsize(file_path)
            except Exception:
                size = 0
        row_count = entry.get("record_count") or entry.get("total_items") or entry.get("row_count") or 0
        data_row_start = entry.get("data_row_start") or 1 if row_count > 0 else 0
        data_row_end = entry.get("data_row_end") or row_count if row_count > 0 else 0
        total_items += row_count
        total_size += size
        meta_files += 1
        summary.append({
            "file_name": file_name,
            "file_path": file_path,
            "file_type": ext,
            "status": status,
            "size_bytes": size,
            "row_count": row_count,
            "data_row_start": data_row_start,
            "data_row_end": data_row_end,
            "last_sync_time": entry.get("last_sync_time"),
            "last_status_change": entry.get("last_status_change"),
            "history": entry.get("history", []),
        })
    t1 = time.time()
    return JSONResponse({
        "total_items": total_items,
        "total_files": meta_files,
        "total_size_bytes": total_size,
        "files": summary,
        "folder_path": DATA_FOLDER,
        "scan_duration_seconds": round(t1 - t0, 3),
        "last_scan_utc": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
    })

8. smart_file_loader.py:

import os
import json
import pandas as pd

# --- CONFIGURABLE ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FOLDER = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_FOLDER, "csvjson_gdrive_meta.json")
ALLOWED_STATUS = {"active", "new", "changed", "done"}
EXCLUDE_FILES = {"file_progress.json", "csvjson_gdrive_meta.json", "other_gdrive_meta.json"}

def is_parquet(fname):
    fname = str(fname).lower()
    return fname.endswith('.parquet') or fname.endswith('.parquet.gz')

def get_valid_meta_files(meta_file=META_FILE, allowed_status=ALLOWED_STATUS):
    """
    Ambil set nama file dari meta yang statusnya valid.
    """
    if not os.path.exists(meta_file):
        return set()
    try:
        with open(meta_file, "r", encoding="utf-8") as f:
            meta = json.load(f)
        return set(
            entry.get("saved_name") or entry.get("name")
            for entry in meta
            if (entry.get("saved_name") or entry.get("name")) and entry.get("status") in allowed_status
        )
    except Exception as e:
        print(f"[smart_file_loader][ERROR] Failed to read meta: {e}")
        return set()

def get_all_parquet_files(data_folder=DATA_FOLDER):
    """
    Scan semua file parquet di folder data (hybrid: meta + folder scan).
    """
    files = set()
    for fname in os.listdir(data_folder):
        if fname in EXCLUDE_FILES:
            continue
        if is_parquet(fname):
            files.add(fname)
    return files

def load_all_parquet_tables(
    data_folder=DATA_FOLDER,
    meta_file=META_FILE,
    allowed_status=ALLOWED_STATUS,
    exclude_files=EXCLUDE_FILES
):
    """
    Loader hybrid, meta-centric:
      - File dari meta (status valid) selalu diproses.
      - File baru di folder/rename tetap terdeteksi otomatis.
      - Tidak perlu revisi kode saat file berubah.
    """
    meta_files = get_valid_meta_files(meta_file, allowed_status)
    folder_files = get_all_parquet_files(data_folder)
    all_files = meta_files | folder_files  # union: semua file yang valid di meta dan file baru di folder
    tables = {}
    for fname in sorted(all_files):
        if fname in exclude_files:
            continue
        fpath = os.path.join(data_folder, fname)
        if not os.path.exists(fpath):
            continue
        try:
            if fname.endswith('.parquet.gz'):
                import gzip
                with gzip.open(fpath, 'rb') as gzfile:
                    df = pd.read_parquet(gzfile)
            else:
                df = pd.read_parquet(fpath)
            columns = list(df.columns)
            data = df.fillna('').to_dict(orient='records')
            tables[fname] = {
                "columns": columns,
                "data": data,
                "in_meta": fname in meta_files,
            }
            if fname not in meta_files:
                print(f"[SMART-LOADER][WARNING] File {fname} belum tercatat di meta! (akan diproses, meta perlu update jika ingin full meta-centric)")
        except Exception as e:
            print(f"[SMART-LOADER][ERROR] Gagal load {fname}: {e}")
    print(f"[SMART-LOADER] Total file terbaca: {len(tables)}")
    return tables

def get_first_parquet_file_path(data_folder=DATA_FOLDER, table_name=None):
    """
    Dapatkan path file parquet pertama untuk table_name, prioritas .parquet.gz lalu .parquet.
    """
    PRIORITY_EXTS = ['.parquet.gz', '.parquet']
    files = get_all_parquet_files(data_folder)
    if table_name:
        # Toleran terhadap spasi dan case
        norm_table = table_name.strip().lower().replace(" ", "")
        for ext in PRIORITY_EXTS:
            for f in files:
                fname_noext, fext = os.path.splitext(f)
                if fname_noext.strip().lower().replace(" ", "") == norm_table and fext.lower() == ext:
                    fpath = os.path.join(data_folder, f)
                    return fpath, f, get_media_type(f)
    for ext in PRIORITY_EXTS:
        for f in files:
            if f.lower().endswith(ext):
                fpath = os.path.join(data_folder, f)
                return fpath, f, get_media_type(f)
    return None, None, None

def get_media_type(fname):
    fname = fname.lower()
    if fname.endswith('.parquet.gz'):
        return "application/gzip"
    elif fname.endswith('.parquet'):
        return "application/octet-stream"
    else:
        return "application/octet-stream"

class SmartFileLoader:
    def __init__(self, data_folder=DATA_FOLDER):
        self.data_folder = data_folder

    @staticmethod
    def supported_formats():
        return [
            ".parquet", ".parquet.gz"
        ]

    def load_all_parquet_tables(self):
        return load_all_parquet_tables(self.data_folder)

    def get_first_parquet_file_path(self, table_name=None):
        return get_first_parquet_file_path(self.data_folder, table_name)

    def get_media_type(self, fname):
        return get_media_type(fname)

# Contoh penggunaan (standalone):
if __name__ == "__main__":
    tables = load_all_parquet_tables()
    for fname, table in tables.items():
        print(f"{fname}: {len(table['data'])} rows, columns={table['columns']}, in_meta={table['in_meta']}")

9. smart_file_preprocessing.py:

import os
import json
from typing import List, Dict

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FOLDER = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_FOLDER, "csvjson_gdrive_meta.json")
ALLOWED_STATUS = {"active", "new", "changed", "done"}

def load_valid_parquet_files_from_meta(meta_file=META_FILE, allowed_status=ALLOWED_STATUS, data_folder=DATA_FOLDER):
    """
    Ambil daftar file Parquet yang statusnya valid dari meta.
    """
    if not os.path.exists(meta_file):
        print(f"[smart_file_preprocessing] Meta file {meta_file} tidak ditemukan!")
        return []
    try:
        with open(meta_file, "r", encoding="utf-8") as f:
            meta = json.load(f)
        valid_files = []
        for entry in meta:
            fname = entry.get("saved_name") or entry.get("name")
            status = entry.get("status", "")
            if fname and status in allowed_status and fname.lower().endswith('.parquet'):
                fpath = os.path.join(data_folder, fname)
                if os.path.exists(fpath):
                    valid_files.append((fname, fpath))
        return valid_files
    except Exception as e:
        print(f"[smart_file_preprocessing][ERROR] Gagal baca meta: {e}")
        return []

def extract_parquet_lines(filepath: str) -> List[str]:
    """
    Membaca semua baris dari file Parquet sebagai string (setiap baris -> JSON string).
    """
    try:
        import pandas as pd
        df = pd.read_parquet(filepath)
        lines = []
        for _, row in df.iterrows():
            # Gabungkan semua kolom jadi satu string, atau bisa diubah ke dict/JSON.
            line = json.dumps(row.dropna().to_dict(), ensure_ascii=False)
            lines.append(line)
        print(f"[DEBUG] extract_parquet_lines: extracted {len(lines)} lines from {filepath}")
        return lines
    except Exception as e:
        print(f"[smart_file_preprocessing][ERROR] Gagal extract_parquet_lines for {filepath}: {e}")
        return []

def preprocess_all_files(data_folder: str = DATA_FOLDER) -> Dict[str, Dict]:
    """
    Returns a dict: {filename: {"raw_lines": [...], "extension": ext}}
    Hanya proses file Parquet yang ada di meta dengan status valid.
    """
    print(f"[DEBUG] preprocess_all_files: processing folder {data_folder}")
    data = {}
    files = load_valid_parquet_files_from_meta(data_folder=data_folder)
    for fname, fpath in files:
        ext = os.path.splitext(fname)[-1].lower()
        print(f"[DEBUG] preprocess_all_files: extracting lines from {fname}")
        raw_lines = extract_parquet_lines(fpath)
        data[fname] = {
            "raw_lines": raw_lines,
            "extension": ext
        }
        print(f"[DEBUG] preprocess_all_files: {fname} -> {len(raw_lines)} lines, ext={ext}")
    print(f"[DEBUG] preprocess_all_files: processed {len(data)} files")
    return data

def preprocess_to_flat_table(pre_file_result: Dict[str, Dict]) -> Dict[str, Dict]:
    """
    Mengubah hasil preprocess_all_files menjadi format tabel flat list of dict,
    seragam dengan output CSV loader: {filename: {"columns": [...], "data": [...]}}
    Untuk Parquet: setiap baris dijadikan dict, kolom diambil dari file.
    """
    print("[DEBUG] preprocess_to_flat_table called")
    result = {}
    for fname, item in pre_file_result.items():
        lines = item.get("raw_lines", [])
        ext = item.get("extension", "")
        # Setiap line sudah dalam format JSON string, ubah ke dict
        data = []
        columns = set()
        for line in lines:
            try:
                row = json.loads(line)
                if isinstance(row, dict):
                    data.append(row)
                    columns.update(row.keys())
            except Exception:
                data.append({"text": line})
                columns.add("text")
        columns = sorted(list(columns))
        result[fname] = {
            "columns": columns,
            "data": data
        }
        print(f"[DEBUG] preprocess_to_flat_table: {fname} -> {len(data)} rows, columns={columns}")
    print(f"[DEBUG] preprocess_to_flat_table: processed {len(result)} files")
    return result

10. smart_file_scanner.py:

import os
import hashlib
import time
import json

# Only Parquet supported via meta after data pipeline migration
SUPPORTED_EXTS = [
    '.parquet', '.parquet.gz'
]
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_DIR, "csvjson_gdrive_meta.json")
ALLOWED_STATUS = {"active", "new", "changed", "done"}

def calc_sha256_from_file(path, block_size=65536):
    """Hitung SHA256 file, efisien untuk file besar."""
    sha256 = hashlib.sha256()
    try:
        print(f"[DEBUG] calc_sha256_from_file: {path}")
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(block_size), b""):
                sha256.update(chunk)
        sha = sha256.hexdigest()
        print(f"[DEBUG] calc_sha256_from_file: {path} sha256={sha}")
        return sha
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] calc_sha256_from_file failed for {path}: {e}")
        return ""

def load_valid_parquet_meta(meta_file=META_FILE, allowed_status=ALLOWED_STATUS, data_dir=DATA_DIR):
    """
    Ambil daftar file Parquet valid dari meta beserta metadata status.
    """
    if not os.path.exists(meta_file):
        print(f"[smart_file_scanner][ERROR] Meta file {meta_file} tidak ditemukan!")
        return []
    try:
        with open(meta_file, "r", encoding="utf-8") as f:
            meta = json.load(f)
        result = []
        for entry in meta:
            fname = entry.get("saved_name") or entry.get("name")
            status = entry.get("status", "")
            if fname and status in allowed_status and fname.lower().endswith('.parquet'):
                fpath = os.path.join(data_dir, fname)
                if os.path.exists(fpath):
                    info = {
                        'name': fname,
                        'path': fpath,
                        'ext': os.path.splitext(fname)[-1].lower(),
                        'meta_status': status,
                        'size_bytes': None,
                        'modified_time': None,
                        'sha256': None,
                        'meta': entry  # simpan meta asli untuk downstream
                    }
                    try:
                        info['size_bytes'] = os.path.getsize(fpath)
                        info['modified_time'] = os.path.getmtime(fpath)
                        info['sha256'] = calc_sha256_from_file(fpath)
                    except Exception as e:
                        print(f"[smart_file_scanner][ERROR] Failed to stat {fname}: {e}")
                    result.append(info)
        return result
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] Gagal baca meta: {e}")
        return []

def scan_data_folder(data_dir=DATA_DIR, exts=SUPPORTED_EXTS, include_hidden=False, pm=None, only_incomplete=False):
    """
    Scan folder data, tapi hanya mengembalikan file Parquet yang ada di meta (status valid).
    Return: list of dict:
        [{
            'name': 'namafile.parquet',
            'path': '/full/path/namafile.parquet',
            'ext': '.parquet',
            'size_bytes': 12345,
            'modified_time': 1685420000.123,
            'sha256': '...',
            'meta_status': 'active',
            'meta': { ... },
            'progress': {...},
            'percent_processed': ..
        }, ...]
    """
    print(f"[DEBUG] scan_data_folder [META MODE]: data_dir={data_dir}, exts={exts}, include_hidden={include_hidden}")
    files = load_valid_parquet_meta(data_dir=data_dir)
    filtered = []
    for info in files:
        fname = info['name']
        if not include_hidden and fname.startswith('.'):
            print(f"[DEBUG] scan_data_folder: skip hidden {fname}")
            continue
        ext = info['ext']
        if ext not in exts:
            print(f"[DEBUG] scan_data_folder: skip ext {fname} ({ext})")
            continue
        # PATCH: Tambahkan info progres dan persen processed jika pm diberikan
        progress = None
        percent_processed = None
        if pm is not None and hasattr(pm, "get_file_progress"):
            progress = pm.get_file_progress(fname)
            if progress:
                info['progress'] = progress
                processed = progress.get('processed', 0)
                total = progress.get('total', None)
                if total and total > 0:
                    percent_processed = round((processed / total) * 100, 2)
                    info['percent_processed'] = percent_processed
                else:
                    info['percent_processed'] = None
        filtered.append(info)
        print(f"[DEBUG] scan_data_folder: found {info}")
    # PATCH: filter only_incomplete jika diinginkan (dan pm ada)
    if only_incomplete and pm is not None:
        before_filter = len(filtered)
        filtered = [
            f for f in filtered
            if f.get("progress") and f["progress"].get("processed", 0) < f["progress"].get("total", 0)
        ]
        print(f"[DEBUG] scan_data_folder: only_incomplete filter: {before_filter} -> {len(filtered)} files")
    print(f"[DEBUG] scan_data_folder: total files found: {len(filtered)}")
    return filtered

def detect_new_and_changed_files(data_dir, prev_snapshot, pm=None, only_incomplete=False):
    """
    Bandingkan snapshot scan terbaru dengan snapshot sebelumnya (list of dict).
    Return: (list_new, list_changed, list_deleted)
    PATCH: mendukung parameter pm & only_incomplete untuk filter file-file active/incomplete.
    """
    print(f"[DEBUG] detect_new_and_changed_files: data_dir={data_dir}")
    try:
        curr_files = scan_data_folder(data_dir=data_dir, pm=pm, only_incomplete=only_incomplete)
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] scan_data_folder error in detect_new_and_changed_files: {e}")
        curr_files = []
    prev_map = {f['name']: f for f in prev_snapshot}
    curr_map = {f['name']: f for f in curr_files}

    new_files = [f for f in curr_files if f['name'] not in prev_map]
    changed_files = [
        f for f in curr_files
        if f['name'] in prev_map and (
            f['sha256'] != prev_map[f['name']]['sha256'] or
            f['modified_time'] != prev_map[f['name']]['modified_time']
        )
    ]
    deleted_files = [f for f in prev_snapshot if f['name'] not in curr_map]

    print(f"[DEBUG] detect_new_and_changed_files: new_files={len(new_files)}, changed_files={len(changed_files)}, deleted_files={len(deleted_files)}")
    return new_files, changed_files, deleted_files

def snapshot_to_dict(snapshot):
    """Convert snapshot list to dict {name: fileinfo}."""
    try:
        d = {f['name']: f for f in snapshot}
        print(f"[DEBUG] snapshot_to_dict: keys={list(d.keys())}")
        return d
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] snapshot_to_dict failed: {e}")
        return {}

if __name__ == "__main__":
    try:
        scan = scan_data_folder()
        print("[smart_file_scanner] Files scanned (META MODE):")
        for info in scan:
            print(info)
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] main scan failed: {e}")

11. sync_progress.py:

import os
import json
from progress_manager import ProgressManager

# CONFIGURABLE: Pastikan DATA_DIR sesuai dengan lokasi file Parquet dan progress Anda
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, "data"))
META_FILE = os.path.join(DATA_DIR, "csvjson_gdrive_meta.json")
ALLOWED_STATUS = {"active", "new", "changed", "done"}

def load_valid_files_from_meta(meta_file, allowed_status=ALLOWED_STATUS):
    """
    Kembalikan set semua nama file dengan status yang valid dari meta agentic (hasil rewrite/overwrite).
    """
    if not os.path.exists(meta_file):
        print(f"[sync_progress] Meta file {meta_file} tidak ditemukan!")
        return set()
    try:
        with open(meta_file, "r", encoding="utf-8") as f:
            meta = json.load(f)
        valid_files = set()
        for entry in meta:
            fname = entry.get("saved_name") or entry.get("name")
            status = entry.get("status", "")
            if fname and status in allowed_status and fname.lower().endswith('.parquet'):
                valid_files.add(fname)
        return valid_files
    except Exception as e:
        print(f"[sync_progress][ERROR] Gagal baca meta: {e}")
        return set()

def agentic_sync_progress(pm: ProgressManager, meta_file: str, allowed_status=ALLOWED_STATUS):
    """
    AGENTIC: Sinkronisasi progress hanya untuk file yang masih valid di meta (meta sudah rewrite/overwrite).
    - Tidak pernah append progress untuk file zombie/orphan.
    - Hapus progress untuk file yang sudah tidak valid/terhapus dari meta.
    - Progress yang valid hanya untuk file .parquet yang statusnya allowed_status.
    """
    valid_files = load_valid_files_from_meta(meta_file, allowed_status)
    pm.sync_progress_with_files()  # Update progress file baru/hilang di folder
    all_progress = pm.get_all_progress()
    to_delete = [fname for fname in all_progress if fname not in valid_files]
    for fname in to_delete:
        pm.delete_progress(fname)
        print(f"[sync_progress] Progress untuk file tidak valid dihapus: {fname}")
    print("[sync_progress] Sinkronisasi progress selesai.")
    print(f"[sync_progress] Status progress terbaru: {pm.get_all_progress()}")
    return pm.get_all_progress()

if __name__ == "__main__":
    try:
        print(f"[sync_progress] Menggunakan DATA_DIR: {DATA_DIR}")
        pm = ProgressManager(data_dir=DATA_DIR)
        agentic_sync_progress(pm, META_FILE)
    except Exception as e:
        print(f"[HYBRID-FALLBACK] ProgressManager sync error: {e}")
        # Fallback: No-op, progress will be synced in the next process run if needed

12. universal_to_parquet_api.py:

from fastapi import APIRouter
from pathlib import Path
import os
import duckdb
import pandas as pd
import shutil
import json
import time

router = APIRouter()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_DIR, "csvjson_gdrive_meta.json")
IGNORE_JSONS = {
    "csvjson_gdrive_meta.json",
    "file_progress.json",
    "other_gdrive_meta.json"
}
ALLOWED_STATUS = {"active", "new", "changed", "done"}

# --- AGENTIC PRUNE META ---
def agentic_prune_meta(meta_path, data_dir, keep_status=ALLOWED_STATUS):
    """
    Prune meta secara agentic: hanya entry yang statusnya valid DAN file-nya benar-benar ADA (di lokal).
    Tidak hanya dedup, tapi juga hapus entry zombie/orphan.
    """
    if not os.path.exists(meta_path):
        print(f"[AGENTIC][PRUNE][WARNING] Meta file {meta_path} tidak ditemukan.")
        return {"status": "failed", "reason": f"Meta file {meta_path} tidak ditemukan."}
    real_files = set(f.lower() for f in os.listdir(data_dir))
    with open(meta_path, "r", encoding="utf-8") as f:
        try:
            meta = json.load(f)
        except Exception as e:
            print(f"[AGENTIC][PRUNE][ERROR] Gagal load meta: {e}")
            return {"status": "failed", "reason": str(e)}
    pruned_meta = []
    seen = set()
    for entry in meta:
        fname = (entry.get("saved_name") or entry.get("name") or "").lower()
        status = entry.get("status", "")
        if status in keep_status and fname in real_files and fname not in seen:
            pruned_meta.append(entry)
            seen.add(fname)
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(pruned_meta, f, indent=2, ensure_ascii=False)
    print(f"[AGENTIC][PRUNE] Meta file pruned+rewrite: {meta_path} ({len(meta)} → {len(pruned_meta)} entry).")
    return {
        "status": "success",
        "meta_path": meta_path,
        "pruned_count": len(pruned_meta),
        "original_count": len(meta)
    }

# --- PATCH: Fungsi update_meta_after_conversion AGENTIC ---
def update_meta_after_conversion(meta_path, original_meta, output_files, extra_fields=None):
    """
    Tambahkan/update entri hasil konversi ke meta file, AGENTIC: rewrite, dedup, anti-append.
    """
    # 1. Load meta
    meta = []
    if os.path.exists(meta_path):
        with open(meta_path, 'r', encoding='utf-8') as f:
            try:
                meta = json.load(f)
            except Exception as e:
                print(f"[META][ERROR] Failed to load meta: {e}")
                meta = []
    # 2. Index meta by saved_name for rewrite/dedup
    meta_dict = {m.get("saved_name"): m for m in meta if m.get("saved_name")}
    now_utc = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    for out_file in output_files:
        entry = meta_dict.get(out_file)
        if entry:
            entry["last_status_change"] = now_utc
            entry["modifiedTime"] = original_meta.get("modifiedTime", entry.get("modifiedTime"))
            entry["data_source"] = original_meta.get("data_source", entry.get("data_source"))
            if "status" not in entry or not entry["status"]:
                entry["status"] = "active"
            if extra_fields:
                entry.update(extra_fields)
            if "history" not in entry:
                entry["history"] = []
            entry["history"].append({
                "event": "converted",
                "time": now_utc,
                "source": original_meta.get("name")
            })
            print(f"[META] Updated meta entry for: {out_file}")
        else:
            meta_entry = {
                "original_name": original_meta.get("name"),
                "saved_name": out_file,
                "id": original_meta.get("id"),
                "md5Checksum": original_meta.get("md5Checksum"),
                "modifiedTime": original_meta.get("modifiedTime"),
                "mimeType": original_meta.get("mimeType"),
                "data_source": original_meta.get("data_source"),
                "status": "active",
                "last_status_change": now_utc,
                "history": [
                    {
                        "event": "converted",
                        "time": now_utc,
                        "source": original_meta.get("name")
                    }
                ]
            }
            if extra_fields:
                meta_entry.update(extra_fields)
            meta_dict[out_file] = meta_entry
            print(f"[META] Added meta entry for: {out_file}")
    # 3. Rewrite/prune meta (overwrite, anti-append, anti-zombie)
    meta_clean = list(meta_dict.values())
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(meta_clean, f, indent=2, ensure_ascii=False)
    print(f"[META] Meta rewritten agentic (dedup, anti-append): {meta_path}")
    return True

def get_gdrive_meta_for_file(meta_path, filename):
    if not os.path.exists(meta_path):
        return None
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
        for m in meta:
            if m.get("saved_name") == filename or m.get("original_name") == filename:
                return m
    except Exception as e:
        print(f"[META] Error reading {meta_path}: {e}")
    return None

def is_data_file_to_convert(fname, meta_path=META_FILE):
    ext = Path(fname).suffix.lower()
    if ext == ".parquet":
        return False
    if fname.endswith(".parquet.meta.json"):
        return False
    if ext == ".json" and fname in IGNORE_JSONS:
        return False
    if not os.path.exists(meta_path):
        return False
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
        for m in meta:
            if (m.get("saved_name") == fname or m.get("original_name") == fname) and m.get("status") in ALLOWED_STATUS:
                return True
        return False
    except Exception as e:
        print(f"[META] Error reading {meta_path} for is_data_file_to_convert: {e}")
        return False

def is_json_nested(data):
    def check_nested(obj):
        if isinstance(obj, dict):
            for v in obj.values():
                if isinstance(v, (dict, list)):
                    return True
        if isinstance(obj, list):
            for v in obj:
                if isinstance(v, (dict, list)):
                    return True
        return False
    if isinstance(data, list) and data:
        return check_nested(data[0])
    if isinstance(data, dict):
        return check_nested(data)
    return False

def flatten_json_obj(obj, sep='.'):
    out = {}
    def _flatten(x, name=''):
        if isinstance(x, dict):
            for k in x:
                _flatten(x[k], f"{name}{k}{sep}")
        elif isinstance(x, list):
            for i, v in enumerate(x):
                _flatten(v, f"{name}{i}{sep}")
        else:
            out[name[:-1]] = x
    _flatten(obj)
    return out

def flatten_json_data(data):
    if isinstance(data, list):
        return [flatten_json_obj(d) for d in data]
    elif isinstance(data, dict):
        return [flatten_json_obj(data)]
    else:
        return []

def duckdb_export(input_path, output_path):
    ext = Path(input_path).suffix.lower()
    try:
        if ext == ".csv":
            duckdb.sql(f"COPY (SELECT * FROM read_csv_auto('{input_path}')) TO '{output_path}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');")
        elif ext == ".tsv":
            duckdb.sql(f"COPY (SELECT * FROM read_csv_auto('{input_path}', delim='\t')) TO '{output_path}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');")
        elif ext == ".feather":
            duckdb.sql(f"COPY (SELECT * FROM read_feather('{input_path}')) TO '{output_path}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');")
        elif ext in {".xls", ".xlsx"}:
            duckdb.sql(f"COPY (SELECT * FROM read_excel('{input_path}')) TO '{output_path}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');")
        elif ext == ".json":
            duckdb.sql(f"COPY (SELECT * FROM read_json_auto('{input_path}')) TO '{output_path}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');")
        else:
            duckdb.sql(f"COPY (SELECT * FROM read_csv_auto('{input_path}')) TO '{output_path}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');")
        print(f"[DUCKDB] Success converting {input_path} to {output_path}")
        return True
    except Exception as e:
        print(f"[DUCKDB] Error converting {input_path} to {output_path}: {e}")
        return False

def fallback_in2csv(filepath, output_parquet_path):
    import tempfile
    import subprocess
    if not shutil.which('in2csv'):
        raise RuntimeError("in2csv (csvkit) belum terinstall. Install dulu dengan: pip install csvkit")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as temp_csv:
        result = subprocess.run(
            ["in2csv", filepath],
            stdout=temp_csv,
            stderr=subprocess.PIPE,
            shell=False
        )
        temp_csv_path = temp_csv.name

    if result.returncode == 0:
        try:
            df = pd.read_csv(temp_csv_path)
            df.to_parquet(output_parquet_path, index=False, compression="snappy")
            print(f"    [FALLBACK in2csv] Saved to {output_parquet_path}")
            return output_parquet_path
        except Exception as e:
            print(f"    [FALLBACK in2csv] Failed to create parquet: {e}")
            return None
        finally:
            os.remove(temp_csv_path)
    else:
        print(f"    [FALLBACK in2csv] {filepath}: {result.stderr.decode()}")
        os.remove(temp_csv_path)
        return None

def save_parquet_meta(parquet_path, source_file_path):
    meta_json_candidates = ["csvjson_gdrive_meta.json", "other_gdrive_meta.json"]
    meta_found = False
    for meta_json in meta_json_candidates:
        meta_path = os.path.join(DATA_DIR, meta_json)
        if os.path.exists(meta_path):
            try:
                with open(meta_path, "r", encoding="utf-8") as f:
                    meta = json.load(f)
                base = os.path.basename(source_file_path)
                matched = [m for m in meta if m.get("original_name") == base or m.get("saved_name") == base]
                if matched:
                    keys = ["original_name", "saved_name", "id", "md5Checksum", "modifiedTime", "mimeType", "data_source"]
                    meta_out = {k: matched[0].get(k) for k in keys if k in matched[0]}
                    with open(parquet_path + ".meta.json", "w", encoding="utf-8") as w:
                        json.dump(meta_out, w, indent=2)
                    print(f"[PARQUET META] Saved meta for {parquet_path}: {meta_out}")
                    meta_found = True
                    break
            except Exception as e:
                print(f"[PARQUET META] Error for {source_file_path}: {e}")
    if not meta_found:
        try:
            with open(parquet_path + ".meta.json", "w", encoding="utf-8") as w:
                json.dump({"original_name": os.path.basename(source_file_path)}, w, indent=2)
            print(f"[PARQUET META] Saved fallback meta for {parquet_path}")
        except Exception as e:
            print(f"[PARQUET META] Fallback error: {e}")

def universal_to_parquet(filepath, output_dir=None, remove_source=False, meta_path=META_FILE):
    import warnings
    warnings.filterwarnings("ignore")
    ext = Path(filepath).suffix.lower()
    fname = os.path.basename(filepath)
    name_noext = os.path.splitext(fname)[0]
    if output_dir is None:
        output_dir = os.path.dirname(filepath)
    outputs = []

    print(f"  [universal_to_parquet] Processing: {fname} (ext: {ext})")
    if meta_path is None:
        meta_path = META_FILE
    gdrive_meta = get_gdrive_meta_for_file(meta_path, fname)

    # --- JSON: Smart Flatten ---
    if ext == ".json":
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                raw = pd.read_json(f, typ='series' if fname.endswith('json') else 'frame', lines=True)
        except Exception as e:
            print(f"    [DEBUG] Error pd.read_json (not fatal): {e}")
            raw = None
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
            if is_json_nested(data):
                print(f"    [FLATTEN] {fname} detected as nested JSON, flattening...")
                flat_data = flatten_json_data(data)
                if flat_data:
                    df = pd.DataFrame(flat_data)
                    outpath = os.path.join(output_dir, f"{fname}.parquet")
                    df.to_parquet(outpath, index=False, compression="snappy")
                    outputs.append(outpath)
                    save_parquet_meta(outpath, filepath)
                    if gdrive_meta:
                        update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(outpath), os.path.basename(outpath) + ".meta.json"])
                    if remove_source:
                        os.remove(filepath)
                    print(f"    [FLATTEN] Flattened JSON saved to {outpath}")
                    return outputs
            outpath = os.path.join(output_dir, f"{fname}.parquet")
            if duckdb_export(filepath, outpath):
                outputs.append(outpath)
                save_parquet_meta(outpath, filepath)
                if gdrive_meta:
                    update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(outpath), os.path.basename(outpath) + ".meta.json"])
                if remove_source:
                    os.remove(filepath)
                return outputs
        except Exception as e:
            print(f"    [JSON ERROR] {filepath}: {e}")

    # --- File tabular mainstream: DuckDB super cepat ---
    if ext in {".csv", ".tsv", ".xls", ".xlsx", ".xlsb", ".feather"}:
        outpath = os.path.join(output_dir, f"{fname}.parquet")
        if duckdb_export(filepath, outpath):
            outputs.append(outpath)
            save_parquet_meta(outpath, filepath)
            if gdrive_meta:
                update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(outpath), os.path.basename(outpath) + ".meta.json"])
            if remove_source:
                os.remove(filepath)
            return outputs

    # --- PDF: extract table ---
    if ext == ".pdf":
        pdf_tables = []
        try:
            import pdfplumber
            with pdfplumber.open(filepath) as pdf:
                for page in pdf.pages:
                    table = page.extract_table()
                    if table and len(table) > 1:
                        df = pd.DataFrame(table[1:], columns=table[0])
                        if not df.empty:
                            pdf_tables.append(df)
            for idx, df in enumerate(pdf_tables):
                pdf_out = os.path.join(output_dir, f"{name_noext}.plumber{idx+1}.parquet")
                df.to_parquet(pdf_out, index=False, compression="snappy")
                outputs.append(pdf_out)
                print(f"    [PDFPLUMBER] Saved table page {idx+1} to {pdf_out}")
            if outputs:
                save_parquet_meta(outputs[0], filepath)
                if gdrive_meta:
                    update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(outputs[0]), os.path.basename(outputs[0]) + ".meta.json"])
        except Exception as e:
            print(f"    [PDFPLUMBER] {filepath}: {e}")
        if not outputs:
            try:
                import tabula
                dfs = tabula.read_pdf(filepath, pages="all", multiple_tables=True, guess=True, lattice=True)
                for idx, df in enumerate(dfs):
                    if not df.empty:
                        pdf_out = os.path.join(output_dir, f"{name_noext}.tabula{idx+1}.parquet")
                        df.to_parquet(pdf_out, index=False, compression="snappy")
                        outputs.append(pdf_out)
                        print(f"    [TABULA] Saved table {idx+1} to {pdf_out}")
                if outputs:
                    save_parquet_meta(outputs[0], filepath)
                    if gdrive_meta:
                        update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(outputs[0]), os.path.basename(outputs[0]) + ".meta.json"])
            except Exception as e:
                print(f"    [TABULA] {filepath}: {e}")
        if outputs:
            if remove_source:
                os.remove(filepath)
            return outputs

    # --- DOCX: extract table ---
    if ext == ".docx":
        try:
            import docx
            document = docx.Document(filepath)
            for idx, table in enumerate(document.tables):
                data = [[cell.text for cell in row.cells] for row in table.rows]
                if data:
                    if len(data) > 1:
                        df = pd.DataFrame(data[1:], columns=data[0])
                    else:
                        df = pd.DataFrame(data)
                    docx_out = os.path.join(output_dir, f"{name_noext}.table{idx+1}.parquet")
                    df.to_parquet(docx_out, index=False, compression="snappy")
                    outputs.append(docx_out)
                    print(f"    [DOCX] Saved table {idx+1} to {docx_out}")
            if outputs:
                save_parquet_meta(outputs[0], filepath)
                if gdrive_meta:
                    update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(outputs[0]), os.path.basename(outputs[0]) + ".meta.json"])
            if outputs and remove_source:
                os.remove(filepath)
            return outputs
        except Exception as e:
            print(f"    [DOCX] {filepath}: {e}")

    # --- TXT: try DuckDB then Pandas fallback ---
    if ext == ".txt":
        outpath = os.path.join(output_dir, f"{fname}.parquet")
        if duckdb_export(filepath, outpath):
            outputs.append(outpath)
            save_parquet_meta(outpath, filepath)
            if gdrive_meta:
                update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(outpath), os.path.basename(outpath) + ".meta.json"])
            if remove_source:
                os.remove(filepath)
            return outputs
        try:
            df = pd.read_csv(filepath, sep="\t")
            if df.shape[1] > 1:
                txt_out = os.path.join(output_dir, f"{name_noext}.tab.parquet")
                df.to_parquet(txt_out, index=False, compression="snappy")
                outputs.append(txt_out)
                save_parquet_meta(txt_out, filepath)
                if gdrive_meta:
                    update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(txt_out), os.path.basename(txt_out) + ".meta.json"])
                if remove_source:
                    os.remove(filepath)
                print(f"    [TXT] Saved tab-parsed to {txt_out}")
                return outputs
        except Exception:
            pass
        try:
            df = pd.read_csv(filepath)
            txt_out = os.path.join(output_dir, f"{name_noext}.comma.parquet")
            df.to_parquet(txt_out, index=False, compression="snappy")
            outputs.append(txt_out)
            save_parquet_meta(txt_out, filepath)
            if gdrive_meta:
                update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(txt_out), os.path.basename(txt_out) + ".meta.json"])
            if remove_source:
                os.remove(filepath)
            print(f"    [TXT] Saved comma-parsed to {txt_out}")
            return outputs
        except Exception:
            pass

    # --- IMAGE: OCR Table Extraction (Very Experimental) ---
    if ext in {".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff"}:
        try:
            import pytesseract
            from PIL import Image
            img = Image.open(filepath)
            text = pytesseract.image_to_string(img)
            rows = [row for row in text.splitlines() if row.strip()]
            table = [r.split('\t') if '\t' in r else r.split(',') for r in rows]
            if table and len(table) > 1:
                df = pd.DataFrame(table[1:], columns=table[0])
                img_out = os.path.join(output_dir, f"{name_noext}.ocr.parquet")
                df.to_parquet(img_out, index=False, compression="snappy")
                outputs.append(img_out)
                save_parquet_meta(img_out, filepath)
                if gdrive_meta:
                    update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(img_out), os.path.basename(img_out) + ".meta.json"])
                if remove_source:
                    os.remove(filepath)
                print(f"    [IMAGE-OCR] Saved to {img_out}")
                return outputs
        except Exception as e:
            print(f"    [IMAGE-OCR] {filepath}: {e}")

    # --- Fallback: csvkit in2csv (for odd formats) ---
    try:
        outpath = os.path.join(output_dir, f"{fname}.parquet")
        fallback_result = fallback_in2csv(filepath, outpath)
        if fallback_result:
            outputs.append(fallback_result)
            save_parquet_meta(fallback_result, filepath)
            if gdrive_meta:
                update_meta_after_conversion(meta_path, gdrive_meta, [os.path.basename(fallback_result), os.path.basename(fallback_result) + ".meta.json"])
            if remove_source:
                os.remove(filepath)
            return outputs
    except Exception as e:
        print(f"    [FALLBACK in2csv] {filepath}: {e}")

    print(f"  [universal_to_parquet] {fname}: No conversion performed.")
    return outputs

def convert_all_non_parquet_to_parquet(data_dir=DATA_DIR):
    # AGENTIC PRUNE: SELALU prune meta sebelum batch konversi!
    agentic_prune_meta(META_FILE, data_dir)
    print("DATA_DIR:", data_dir)
    files = os.listdir(data_dir)
    print("Files in data_dir:", files)
    results = []
    meta_path = os.path.join(data_dir, "csvjson_gdrive_meta.json")
    for fname in files:
        print("Checking file:", fname)
        if not is_data_file_to_convert(fname, meta_path=meta_path):
            print("Skipping file (filtered by meta or extension):", fname)
            continue
        fpath = os.path.join(data_dir, fname)
        if not os.path.isfile(fpath):
            print("Skipping non-file:", fname)
            continue
        try:
            print("Processing file:", fname)
            parquet_outputs = universal_to_parquet(fpath, output_dir=data_dir, remove_source=False, meta_path=meta_path)
            print("Result for", fname, ":", parquet_outputs)
            if parquet_outputs:
                results.append({"file": fname, "parquet_outputs": parquet_outputs})
            else:
                results.append({"file": fname, "info": "No table extracted or already in Parquet"})
        except Exception as e:
            print("Error processing", fname, ":", e)
            results.append({"file": fname, "error": str(e)})
    print("FINAL RESULTS:", results)
    
    # PATCH: Jangan hapus file .parquet.meta.json (selain .parquet dan IGNORE_JSONS)
    for fname in os.listdir(data_dir):
        ext = Path(fname).suffix.lower()
        if ext == ".parquet":
            continue
        if fname.endswith(".parquet.meta.json"):
            continue
        if fname in IGNORE_JSONS:
            continue
        # PATCH: Only delete if file is in meta with allowed status
        if not is_data_file_to_convert(fname, meta_path=meta_path):
            continue
        fpath = os.path.join(data_dir, fname)
        try:
            print(f"Deleting source file: {fname}")
            os.remove(fpath)
        except Exception as e:
            print(f"Failed to delete {fname}: {e}")

    # AGENTIC PRUNE: SELALU prune meta setelah batch konversi!
    agentic_prune_meta(META_FILE, data_dir)
    return results

@router.post("/convert-all-to-parquet")
def convert_all_files_to_parquet_endpoint():
    """
    Trigger batch conversion: all non-parquet files in DATA_DIR -> parquet+snappy.
    Hanya proses file yang statusnya sesuai di meta.
    """
    results = convert_all_non_parquet_to_parquet()
    return {"status": "done", "result": results}

13. upload_frontend_data.py:

from fastapi import APIRouter, Request
import os
import json
from progress_manager import ProgressManager

router = APIRouter()

DATA_DIR = r"C:\Users\ASUS\kpifinance-api\backend-python\data"
os.makedirs(DATA_DIR, exist_ok=True)

@router.post("/upload_frontend_data")
async def upload_frontend_data(request: Request):
    print("[DEBUG] upload_frontend_data: called")
    data = await request.json()
    print(f"[DEBUG] upload_frontend_data: received data type={type(data)}, keys={list(data.keys()) if isinstance(data, dict) else 'not dict'}")
    filepath = os.path.join(DATA_DIR, "frontend_data.json")
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"[DEBUG] upload_frontend_data: data saved to {filepath}")

    # PATCH: Sinkronisasi progress/meta setelah upload data (hybrid fallback)
    pm = ProgressManager(DATA_DIR)
    try:
        pm.sync_progress_with_files()
        print("[DEBUG] upload_frontend_data: progress/meta sync selesai (hybrid)")
    except Exception as e:
        print(f"[upload_frontend_data][HYBRID-FALLBACK] sync_progress_with_files error: {e}")
        # Fallback: tidak fatal, lanjutkan saja, progress/meta akan disinkronkan otomatis di proses lain

    return {"status": "ok", "saved_to": filepath}

14. utils_gdrive.py:

import os
import json
from datetime import datetime, timezone
from fastapi import APIRouter

router = APIRouter()

# --- CONFIGURABLE ---
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
DEFAULT_META_PREFIX = "csvjson"
SERVICE_ACCOUNT_JSON = os.path.join(os.path.dirname(os.path.abspath(__file__)), "gdrive_service_account_csvjson.json")
GDRIVE_FOLDER_ID = "1nDveh8erKuSFTghCHhlPEEZmu9SUo8jB"

def utc_now_iso():
    return datetime.now(timezone.utc).isoformat()

def get_gdrive_file_list(folder_id, service_account_json_path):
    from googleapiclient.discovery import build
    from google.oauth2 import service_account
    SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
    creds = service_account.Credentials.from_service_account_file(service_account_json_path, scopes=SCOPES)
    service = build('drive', 'v3', credentials=creds)
    query = f"'{folder_id}' in parents and trashed = false"
    meta_files = []
    page_token = None
    while True:
        response = service.files().list(
            q=query,
            spaces='drive',
            fields='nextPageToken, files(id, name, mimeType, md5Checksum, modifiedTime, size)',
            pageToken=page_token
        ).execute()
        files = response.get('files', [])
        for f in files:
            meta_files.append({
                'id': f['id'],
                'name': f['name'],
                'md5Checksum': f.get('md5Checksum'),
                'modifiedTime': f.get('modifiedTime'),
                'mimeType': f.get('mimeType'),
                'size': f.get('size'),
            })
        page_token = response.get('nextPageToken', None)
        if not page_token:
            break
    return meta_files

def scan_local_files(data_dir):
    files = {}
    for fname in os.listdir(data_dir):
        fpath = os.path.join(data_dir, fname)
        if not os.path.isfile(fpath):
            continue
        stat = os.stat(fpath)
        files[fname] = {
            "local_exists": True,
            "local_modified_time": datetime.utcfromtimestamp(stat.st_mtime).isoformat() + "Z",
            "local_size": stat.st_size,
        }
    return files

def rewrite_meta(meta_path, entries):
    """
    Agentic: Overwrite meta file dengan hanya entry terbaru (NO ZOMBIE, anti-duplikat, anti-orphan).
    """
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(entries, f, indent=2)
    print(f"[META][REWRITE][CLEAN] Meta rewritten: {meta_path} ({len(entries)} entries)")

def ensure_gdrive_data(folder_id, data_dir, service_account_json_path, meta_prefix=DEFAULT_META_PREFIX):
    meta_path = os.path.join(data_dir, f"{meta_prefix}_gdrive_meta.json")
    os.makedirs(data_dir, exist_ok=True)

    # 1. List file dari GDrive (source of truth, NO ZOMBIE, NO ORPHAN)
    gdrive_files = get_gdrive_file_list(folder_id, service_account_json_path)
    local_files = scan_local_files(data_dir)
    now = utc_now_iso()

    # 2. Build meta HANYA dari file yang ada di GDrive, plus info lokal jika ada
    meta = []
    for gfile in gdrive_files:
        fname = gfile["name"]
        entry = {
            "saved_name": fname,
            "last_sync_time": now,
            "history": [],
            **gfile,
            "gdrive_exists": True,
        }
        local_info = local_files.get(fname)
        if local_info:
            entry.update(local_info)
            entry["local_exists"] = True
        else:
            entry["local_exists"] = False
        entry["status"] = "active" if entry["local_exists"] else "missing_local"
        entry["status_reason"] = "synced" if entry["local_exists"] else "not_downloaded"
        entry["status_changes"] = []
        entry["last_status_change"] = now
        entry["process_start_time"] = None
        entry["process_end_time"] = now if entry["local_exists"] else None
        meta.append(entry)

    # 3. REWRITE meta file (NO ZOMBIE, NO ORPHAN, anti-duplikat)
    rewrite_meta(meta_path, meta)

    # 4. (Opsional) Jika ingin benar-benar membersihkan orphan file fisik, lakukan manual di luar pipeline agentic.
    # Pipeline agentic sudah cukup dengan overwrite meta.

    return {
        "status": "success",
        "meta_file": meta_path,
        "file_count": len(meta),
        "files": [entry["saved_name"] for entry in meta]
    }

@router.post("/trigger_gdrive_sync")
def trigger_gdrive_sync_endpoint():
    result = ensure_gdrive_data(
        folder_id=GDRIVE_FOLDER_ID,
        data_dir=DATA_DIR,
        service_account_json_path=SERVICE_ACCOUNT_JSON,
        meta_prefix=DEFAULT_META_PREFIX
    )
    return result
